# chat_session.R  ------------------------------------------------------------
# Stateful chat wrapper for LLMR (list-based, no closure trickery)
# ---------------------------------------------------------------------------

## helper: make a well-formed message ---------------------------------------
.msg <- function(role, content) list(role = role, content = as.character(content)[1])

## provider-agnostic token counter ------------------------------------------
.token_counts <- function(j) {
  if (!is.null(j$usage)) {
    u <- j$usage
    if (!is.null(u$prompt_tokens)  && !is.null(u$completion_tokens))
      return(list(sent = u$prompt_tokens, rec = u$completion_tokens))
    if (!is.null(u$input_tokens)   && !is.null(u$output_tokens))
      return(list(sent = u$input_tokens,  rec = u$output_tokens))
  }
  if (!is.null(j$usageMetadata)) {
    m <- j$usageMetadata
    if (!is.null(m$promptTokenCount) && !is.null(m$candidatesTokenCount))
      return(list(sent = m$promptTokenCount, rec = m$candidatesTokenCount))
  }
  list(sent = 0, rec = 0)
}

#' Stateful chat session
#'
#' Create a lightweight, in-memory conversation object that retains message
#' history between calls to the LLM.  Internally it wraps
#' \code{call_llm_robust()} so you still benefit from retry logic,
#' caching, and error logging.
#'
#' @section How it works:
#'   1.  A private environment stores the running list of
#'       \code{list(role, content)} messages.
#'   2.  At each \code{$send()} the history is sent *in full* to the model.
#'   3.  Provider-agnostic token counts are extracted from the JSON response
#'       (fields are detected by name, so new providers continue to work).
#'
#' @param config  An [\code{llm_config}] **for a generative model**
#'                (i.e. \code{embedding = FALSE}).
#' @param system  Optional system prompt inserted once at the beginning.
#' @param ...     Default arguments forwarded to every
#'                [\code{call_llm_robust()}] call (e.g.
#'                \code{verbose = TRUE}, \code{json = TRUE}).
#'
#' @return An object of class **\code{llm_chat_session}** with the methods
#'   listed below.
#'
#' @section Public methods:
#' \describe{
#'   \item{\code{$send(text, ..., role = "user")}}{
#'     Append a message (default role \code{"user"}), query the model,
#'     print the assistantâ€™s reply, and invisibly return it.}
#'   \item{\code{$history()}}{Raw list of messages.}
#'   \item{\code{$history_df()}}{Two-column data frame (\code{role},
#'     \code{content}).}
#'   \item{\code{$tokens_sent()}/\code{$tokens_received()}}{Running token
#'     totals.}
#'   \item{\code{$reset()}}{Clear history (retains the optional system
#'     message).}
#' }
#'
#' @examples
#' \dontrun{
#' cfg  <- llm_config("openai", "gpt-4o-mini", Sys.getenv("OPENAI_API_KEY"))
#' chat <- chat_session(cfg, system = "Be concise.")
#' chat$send("Who invented the moon?")
#' chat$send("Explain why in one short sentence.")
#' chat           # snapshot (first 10 turns)
#' tail(chat, 2)  # last 2 turns
#' }
#' @export
#'
chat_session <- function(config, system = NULL, ...) {

  stopifnot(inherits(config, "llm_config"))
  if (isTRUE(config$embedding))
    stop("chat_session requires a generative model (embedding = FALSE).")

  ## private state ----------------------------------------------------------
  e <- new.env(parent = emptyenv())
  e$messages <- if (is.null(system)) list() else list(.msg("system", system))
  e$raw      <- list()
  e$sent     <- 0
  e$received <- 0
  defaults   <- list(...)

  call_robust <- function(extra = list()) {
    clean <- unname(lapply(e$messages, function(m) .msg(m$role, m$content)))
    do.call(
      call_llm_robust,
      c(list(config   = config,
             messages = clean,
             json     = TRUE),
        modifyList(defaults, extra))
    )
  }

  ## exposed methods --------------------------------------------------------
  send <- function(text, ..., role = "user") {
    e$messages <- append(e$messages, list(.msg(role, text)))

    resp <- call_robust(list(...))
    raw  <- attr(resp, "full_response")
    txt  <- extract_text(raw)

    tc <- .token_counts(raw)
    e$sent     <- e$sent     + tc$sent
    e$received <- e$received + tc$rec
    e$raw      <- append(e$raw, list(raw))

    e$messages <- append(e$messages, list(.msg("assistant", txt)))

    cat(txt, "\n")
    invisible(txt)
  }

  history    <- function()  e$messages
  history_df <- function()  data.frame(
    role    = vapply(e$messages, `[[`, "", "role"),
    content = vapply(e$messages, `[[`, "", "content"),
    stringsAsFactors = FALSE
  )
  tokens_sent      <- function() e$sent
  tokens_received  <- function() e$received
  reset <- function() {
    e$messages <- if (is.null(system)) list() else list(.msg("system", system))
    e$raw <- list(); e$sent <- 0; e$received <- 0
    invisible(NULL)
  }

  structure(
    list(
      send            = send,
      history         = history,
      history_df      = history_df,
      tokens_sent     = tokens_sent,
      tokens_received = tokens_received,
      reset           = reset
    ),
    class = "llm_chat_session"
  )
}

# ---------------------------------------------------------------------------#
# S3 helpers so base verbs behave naturally                                  #
# ---------------------------------------------------------------------------#


#' @describeIn chat_session Coerce a session to a two-column data frame.
#' @export
as.data.frame.llm_chat_session <- function(x, ...) {
  x$history_df()
}

#' @describeIn chat_session Summary statistics for a chat session.
#' @export
summary.llm_chat_session <- function(object, ...) {
  hist <- object$history_df()
  out  <- list(
    turns            = nrow(hist),
    tokens_sent      = object$tokens_sent(),
    tokens_received  = object$tokens_received(),
    last_assistant   = tail(hist$content[hist$role == "assistant"], 1)
  )
  class(out) <- "summary.llm_chat_session"
  out
}

#' @export
print.summary.llm_chat_session <- function(x, ...) {
  cat("llm_chat_session summary\n",
      "-----------------------\n",
      "Turns:            ", x$turns,           "\n",
      "Tokens sent:      ", x$tokens_sent,     "\n",
      "Tokens received:  ", x$tokens_received, "\n",
      "Last assistant:   ", x$last_assistant,  "\n", sep = "")
  invisible(x)
}

# ---------------------------------------------------------------------------#
# Custom print: row-by-row display with truncation                           #
# ---------------------------------------------------------------------------#

## ------------------------------------------------------------------ ##
##  helper: row-by-row pretty printer with truncation                 ##
## ------------------------------------------------------------------ ##
.format_rows <- function(df, width = getOption("width") - 15) {
  for (i in seq_len(nrow(df))) {
    txt <- df$content[i]
    if (nchar(txt) > width)
      txt <- paste0(substr(txt, 1, width - 3), "...")
    cat(sprintf("[%s] %s\n", df$role[i], txt))
  }
}

#' @describeIn chat_session First *n* rows of the conversation.
#' @export
head.llm_chat_session <- function(x, n = 6L, width = getOption("width") - 15, ...) {
  slice <- utils::head(x$history_df(), n, ...)
  .format_rows(slice, width)
  invisible(slice)
}

#' @describeIn chat_session Last *n* rows of the conversation.
#' @export
tail.llm_chat_session <- function(x, n = 6L, width = getOption("width") - 15, ...) {
  slice <- utils::tail(x$history_df(), n, ...)
  .format_rows(slice, width)
  invisible(slice)
}

#' @export
print.llm_chat_session <- function(x, width = getOption("width") - 15, ...) {
  hist <- x$history_df()
  cat("llm_chat_session (turns:", nrow(hist),
      "| sent:", x$tokens_sent(),
      "| rec:",  x$tokens_received(), ")\n\n")

  .format_rows(utils::head(hist, 10), width)
  if (nrow(hist) > 10) cat("...\n")
  invisible(x)
}


# LLM_parallel_utils.R
# -------------------------------------------------------------------
# This file provides parallelized services for dispatching multiple LLM API calls
# concurrently using tibble-based experiment designs. It leverages the 'future'
# package for OS-agnostic parallelization and uses call_llm_robust() as the default
# calling mechanism with configurable retry and delay settings.
#
# Key Features:
#   1. call_llm_sweep() - Parameter sweep mode: vary one parameter, fixed message
#   2. call_llm_broadcast() - Message broadcast mode: fixed config, multiple messages
#   3. call_llm_compare() - Model comparison mode: multiple configs, fixed message
#   4. call_llm_par() - General mode: tibble with config and message columns
#   5. build_factorial_experiments() - Helper for factorial experimental designs
#   6. setup_llm_parallel() / reset_llm_parallel() - Environment management
#   7. Automatic load balancing and error handling
#   8. Progress tracking and detailed logging
#   9. Native metadata support through tibble columns
#  10. Automatic capture of raw JSON API responses
#
# Design Philosophy:
#   All experiment functions use tibbles with list-columns for configs and messages.
#   call_llm_par() is the core parallel engine. Wrapper functions (_sweep, _broadcast,
#   _compare) prepare inputs for call_llm_par() and then use a helper to
#   format the output, including unnesting config parameters.
#   This enables natural use of dplyr verbs for building complex experimental designs.
#   Metadata columns are preserved automatically.
#
# Dependencies: future, future.apply, tibble, dplyr, progressr (optional), tidyr (for build_factorial_experiments)
# -------------------------------------------------------------------

# Internal helper to unnest config details into columns
.unnest_config_to_cols <- function(results_df, config_col = "config") {
  if (!config_col %in% names(results_df) || !is.list(results_df[[config_col]])) {
    warning(paste0("Config column '", config_col, "' not found or not a list-column. Cannot unnest parameters."))
    return(results_df)
  }

  # Extract provider and model
  results_df$provider <- sapply(results_df[[config_col]], function(cfg) cfg$provider %||% NA_character_)
  results_df$model    <- sapply(results_df[[config_col]], function(cfg) cfg$model %||% NA_character_)

  # Extract all model parameters
  all_model_param_names <- unique(unlist(lapply(results_df[[config_col]], function(cfg) names(cfg$model_params))))

  if (length(all_model_param_names) > 0) {
    param_cols_list <- lapply(all_model_param_names, function(p_name) {
      sapply(results_df[[config_col]], function(cfg) {
        val <- cfg$model_params[[p_name]]
        if (is.null(val)) NA else val
      })
    })
    names(param_cols_list) <- all_model_param_names
    params_df <- tibble::as_tibble(param_cols_list)
    results_df <- dplyr::bind_cols(results_df, params_df)
  }

  # Identify metadata columns (everything except standard columns)
  meta_cols <- setdiff(names(results_df), c("provider", "model", all_model_param_names,
                                            config_col, "response_text", "raw_response_json",
                                            "success", "error_message"))

  # Separate standard config columns from other parameters
  standard_config_cols <- c("provider", "model")
  ordered_param_cols <- all_model_param_names[!all_model_param_names %in% standard_config_cols]

  # Define column order: metadata, config info, parameters, results
  final_cols_order <- c(
    meta_cols,
    standard_config_cols,
    ordered_param_cols,
    "response_text", "raw_response_json", "success", "error_message"
  )

  # Only include columns that exist
  final_cols_order_existing <- final_cols_order[final_cols_order %in% names(results_df)]
  remaining_cols <- setdiff(names(results_df), final_cols_order_existing)

  results_df <- results_df[, c(final_cols_order_existing, remaining_cols)]

  return(results_df)
}

#' Mode 1: Parameter Sweep - Vary One Parameter, Fixed Message
#'
#' Sweeps through different values of a single parameter while keeping the message constant.
#' Perfect for hyperparameter tuning, temperature experiments, etc.
#' This function requires setting up the parallel environment using `setup_llm_parallel`.
#'
#' @param base_config Base llm_config object to modify.
#' @param param_name Character. Name of the parameter to vary (e.g., "temperature", "max_tokens").
#' @param param_values Vector. Values to test for the parameter.
#' @param messages A character vector or a list of message objects (same for all calls).
#' @param ... Additional arguments passed to `call_llm_par` (e.g., tries, verbose, progress).
#'
#' @return A tibble with columns: swept_param_name, the varied parameter column, provider, model,
#'   all other model parameters, response_text, raw_response_json, success, error_message.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Temperature sweep
#'   config <- llm_config(provider = "openai", model = "gpt-4o-mini",
#'                        api_key = Sys.getenv("OPENAI_API_KEY"))
#'
#'   messages <- "What is 15 * 23?"
#'   temperatures <- c(0, 0.3, 0.7, 1.0, 1.5)
#'
#'   setup_llm_parallel(workers = 4, verbose = TRUE)
#'   results <- call_llm_sweep(config, "temperature", temperatures, messages)
#'   reset_llm_parallel(verbose = TRUE)
#' }
call_llm_sweep <- function(base_config,
                           param_name,
                           param_values,
                           messages,
                           ...) {

  if (!requireNamespace("tibble", quietly = TRUE)) {
    stop("The 'tibble' package is required. Please install it with: install.packages('tibble')")
  }

  if (length(param_values) == 0) {
    warning("No parameter values provided. Returning empty tibble.")
    return(tibble::tibble(
      swept_param_name = character(0),
      provider = character(0),
      model = character(0),
      response_text = character(0),
      raw_response_json = character(0),
      success = logical(0),
      error_message = character(0)
    ))
  }

  # Build experiments tibble
  experiments <- tibble::tibble(
    .param_name_sweep = param_name,
    .param_value_sweep = param_values,
    config = lapply(param_values, function(val) {
      modified_config <- base_config
      if (is.null(modified_config$model_params)) modified_config$model_params <- list()
      modified_config$model_params[[param_name]] <- val
      modified_config
    }),
    messages = rep(list(messages), length(param_values))
  )

  # Run parallel processing
  results_raw <- call_llm_par(experiments, ...)
  results_final$config <- NULL

  # Create the parameter column with actual name
  results_final[[param_name]] <- results_final$.param_value_sweep
  results_final$swept_param_name <- results_final$.param_name_sweep

  # Remove temporary columns
  results_final$.param_name_sweep <- NULL
  results_final$.param_value_sweep <- NULL

  # Identify column groups
  meta_cols <- setdiff(names(results_final), c("swept_param_name", param_name, "provider", "model",
                                               "response_text", "raw_response_json", "success", "error_message"))

  all_model_param_names_unnested <- setdiff(
    names(results_final)[!names(results_final) %in% c(meta_cols, "swept_param_name", param_name,
                                                      "provider", "model", "response_text",
                                                      "raw_response_json", "success", "error_message")],
    param_name
  )

  # Final column ordering
  final_order <- c("swept_param_name", param_name, meta_cols, "provider", "model",
                   all_model_param_names_unnested,
                   "response_text", "raw_response_json", "success", "error_message")
  final_order_existing <- final_order[final_order %in% names(results_final)]
  remaining_cols <- setdiff(names(results_final), final_order_existing)

  results_final <- results_final[, c(final_order_existing, remaining_cols)]

  return(results_final)
}

#' Mode 2: Message Broadcast - Fixed Config, Multiple Messages
#'
#' Broadcasts different messages using the same configuration in parallel.
#' Perfect for batch processing different prompts with consistent settings.
#' This function requires setting up the parallel environment using `setup_llm_parallel`.
#'
#' @param config Single llm_config object to use for all calls.
#' @param messages A character vector (each element is a prompt) OR
#'   a list where each element is a pre-formatted message list.
#' @param ... Additional arguments passed to `call_llm_par` (e.g., tries, verbose, progress).
#'
#' @return A tibble with columns: message_index (metadata), provider, model,
#'   all model parameters, response_text, raw_response_json, success, error_message.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Broadcast different questions
#'   config <- llm_config(provider = "openai", model = "gpt-4o-mini",
#'                        api_key = Sys.getenv("OPENAI_API_KEY"))
#'
#'   messages <- list(
#'     list(list(role = "user", content = "What is 2+2?")),
#'     list(list(role = "user", content = "What is 3*5?")),
#'     list(list(role = "user", content = "What is 10/2?"))
#'   )
#'
#'   setup_llm_parallel(workers = 4, verbose = TRUE)
#'   results <- call_llm_broadcast(config, messages)
#'   reset_llm_parallel(verbose = TRUE)
#' }
call_llm_broadcast <- function(config,
                               messages,
                               ...) {

  # Allow plain character vectors for convenience
  if (is.character(messages))
    messages <- as.list(messages)

  if (!requireNamespace("tibble", quietly = TRUE)) {
    stop("The 'tibble' package is required. Please install it with: install.packages('tibble')")
  }

  if (length(messages) == 0) {
    warning("No messages provided. Returning empty tibble.")
    return(tibble::tibble(
      message_index = integer(0),
      provider = character(0),
      model = character(0),
      response_text = character(0),
      raw_response_json = character(0),
      success = logical(0),
      error_message = character(0)
    ))
  }

  # Build experiments tibble
  experiments <- tibble::tibble(
    message_index = seq_along(messages),
    config = rep(list(config), length(messages)),
    messages = messages
  )

  # Run parallel processing
  results_raw <- call_llm_par(experiments, ...)
  results_final <- results_raw
  results_final$config <- NULL
  return(results_final)
}

#' Mode 3: Model Comparison - Multiple Configs, Fixed Message
#'
#' Compares different configurations (models, providers, settings) using the same message.
#' Perfect for benchmarking across different models or providers.
#' This function requires setting up the parallel environment using `setup_llm_parallel`.
#'
#' @param configs_list A list of llm_config objects to compare.
#' @param messages A character vector or a list of message objects (same for all configs).
#' @param ... Additional arguments passed to `call_llm_par` (e.g., tries, verbose, progress).
#'
#' @return A tibble with columns: config_index (metadata), provider, model,
#'   all varying model parameters, response_text, raw_response_json, success, error_message.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Compare different models
#'   config1 <- llm_config(provider = "openai", model = "gpt-4o-mini",
#'                         api_key = Sys.getenv("OPENAI_API_KEY"))
#'   config2 <- llm_config(provider = "openai", model = "gpt-3.5-turbo",
#'                         api_key = Sys.getenv("OPENAI_API_KEY"))
#'
#'   configs_list <- list(config1, config2)
#'   messages <- "Explain quantum computing"
#'
#'   setup_llm_parallel(workers = 4, verbose = TRUE)
#'   results <- call_llm_compare(configs_list, messages)
#'   reset_llm_parallel(verbose = TRUE)
#' }
call_llm_compare <- function(configs_list,
                             messages,
                             ...) {
  if (!requireNamespace("tibble", quietly = TRUE)) {
    stop("The 'tibble' package is required. Please install it with: install.packages('tibble')")
  }

  if (length(configs_list) == 0) {
    warning("No configs provided. Returning empty tibble.")
    return(tibble::tibble(
      config_index = integer(0),
      provider = character(0),
      model = character(0),
      response_text = character(0),
      raw_response_json = character(0),
      success = logical(0),
      error_message = character(0)
    ))
  }

  # Build experiments tibble
  experiments <- tibble::tibble(
    config_index = seq_along(configs_list),
    config = configs_list,
    messages = rep(list(messages), length(configs_list))
  )

  # Run parallel processing
  results_raw <- call_llm_par(experiments, ...)
  results_final <- results_raw
  results_final$config <- NULL
  results_final
  return(results_final)
}

#' Parallel LLM Processing with Tibble-Based Experiments (Core Engine)
#'
#' Processes experiments from a tibble where each row contains a config and message pair.
#' This is the core parallel processing function. Metadata columns are preserved.
#' This function requires setting up the parallel environment using `setup_llm_parallel`.
#'
#' @param experiments A tibble/data.frame with required list-columns 'config' (llm_config objects)
#'   and 'messages' (character vector OR message list).
#' @param simplify Whether to cbind 'experiments' to the output data frame or not.
#' @param tries Integer. Number of retries for each call. Default is 10.
#' @param wait_seconds Numeric. Initial wait time (seconds) before retry. Default is 2.
#' @param backoff_factor Numeric. Multiplier for wait time after each failure. Default is 2.
#' @param verbose Logical. If TRUE, prints progress and debug information.
#' @param memoize Logical. If TRUE, enables caching for identical requests.
#' @param max_workers Integer. Maximum number of parallel workers. If NULL, auto-detects.
#' @param progress Logical. If TRUE, shows progress bar.
#' @param json_output Deprecated. Raw JSON string is always included as raw_response_json.
#'                  This parameter is kept for backward compatibility but has no effect.
#'
#' @return A tibble containing all original columns from experiments (metadata, config, messages),
#'   plus new columns: response_text, raw_response_json (the raw JSON string from the API),
#'   success, error_message, duration (in seconds).
#' @export
#'
#' @examples
#' \dontrun{
#'   library(dplyr)
#'   library(tidyr)
#'
#'   # Build experiments with expand_grid
#'   experiments <- expand_grid(
#'     condition = c("control", "treatment"),
#'     model_type = c("small", "large"),
#'     rep = 1:10
#'   ) |>
#'     mutate(
#'       config = case_when(
#'         model_type == "small" ~ list(small_config),
#'         model_type == "large" ~ list(large_config)
#'       ),
#'       messages = case_when(
#'         condition == "control" ~ list(control_messages),
#'         condition == "treatment" ~ list(treatment_messages)
#'       )
#'     )
#'
#'   setup_llm_parallel(workers = 4)
#'   results <- call_llm_par(experiments, progress = TRUE)
#'   reset_llm_parallel()
#'
#'   # All metadata preserved for analysis
#'   results |>
#'     group_by(condition, model_type) |>
#'     summarise(mean_response = mean(as.numeric(response_text), na.rm = TRUE))
#' }
call_llm_par <- function(experiments,
                         simplify = TRUE,
                         tries = 10,
                         wait_seconds = 2,
                         backoff_factor = 2,
                         verbose = FALSE,
                         memoize = FALSE,
                         max_workers = NULL,
                         progress = FALSE,
                         json_output = NULL) {

  if (!is.null(json_output) && verbose) {
    message("Note: The 'json_output' parameter in call_llm_par is deprecated. Raw JSON string is always included as 'raw_response_json'.")
  }

  # Package checks
  if (!requireNamespace("future", quietly = TRUE)) {
    stop("The 'future' package is required for parallel processing. Please install it with: install.packages('future')")
  }
  if (!requireNamespace("future.apply", quietly = TRUE)) {
    stop("The 'future.apply' package is required for parallel processing. Please install it with: install.packages('future.apply')")
  }
  if (progress && !requireNamespace("progressr", quietly = TRUE)) {
    warning("The 'progressr' package is not available. Progress tracking will be disabled.")
    progress <- FALSE
  }
  if (!requireNamespace("tibble", quietly = TRUE)) {
    stop("The 'tibble' package is required. Please install it with: install.packages('tibble')")
  }
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("The 'dplyr' package is required. Please install it with: install.packages('dplyr')")
  }

  # Input validation
  if (!is.data.frame(experiments)) {
    stop("experiments must be a tibble/data.frame")
  }
  if (!all(c("config", "messages") %in% names(experiments))) {
    stop("experiments must have 'config' and 'messages' columns")
  }
  if (nrow(experiments) == 0) {
    warning("No experiments provided. Returning empty input tibble with result columns.")
    return(dplyr::bind_cols(experiments, tibble::tibble(
      response_text = character(0),
      raw_response_json = character(0),
      success = logical(0),
      error_message = character(0)
    )))
  }

  # Validate configs
  for (i in seq_len(nrow(experiments))) {
    if (!inherits(experiments$config[[i]], "llm_config")) {
      stop(sprintf("Row %d 'config' is not an llm_config object.", i))
    }
  }

  # Setup workers
  if (is.null(max_workers)) {
    max_workers <- min(future::availableCores(omit = 1L), nrow(experiments))
    max_workers <- max(1, max_workers)
  }

  current_plan <- future::plan()
  if (verbose) {
    message(sprintf("Setting up parallel execution with %d workers using plan: %s",
                    max_workers, class(current_plan)[1]))
  }
  on.exit(future::plan(current_plan), add = TRUE)
  if (!inherits(current_plan, "FutureStrategy") || inherits(current_plan, "sequential")) {
    future::plan(future::multisession, workers = max_workers)
  }

  if (verbose) {
    n_metadata_cols <- ncol(experiments) - 2
    message(sprintf("Processing %d experiments with %d user metadata columns",
                    nrow(experiments), n_metadata_cols))
  }

  # Worker function
  par_worker <- function(i_val) {
    start_time <- Sys.time()                 # begin timer
    current_config <- experiments$config[[i_val]]
    current_messages <- experiments$messages[[i_val]]
    raw_json_str <- NA_character_

    tryCatch({
      # Always call with json=TRUE to get attributes for raw_json
      result_content <- call_llm_robust(
        config = current_config,
        messages = current_messages,
        tries = tries,
        wait_seconds = wait_seconds,
        backoff_factor = backoff_factor,
        verbose = FALSE,
        json = TRUE, # Force TRUE to get raw_json attribute
        memoize = memoize
      )

      # Extract raw JSON from attributes
      raw_json_str <- attr(result_content, "raw_json") %||% NA_character_

      list(
        row_index = i_val,
        response_text = as.character(result_content), # Strip attributes
        raw_response_json = raw_json_str,
        success = TRUE,
        error_message = NA_character_,
        duration = as.numeric(difftime(Sys.time(), start_time, units = "secs"))
      )
    }, error = function(e) {
      list(
        row_index = i_val,
        response_text = NA_character_,
        raw_response_json = raw_json_str,
        success = FALSE,
        error_message = conditionMessage(e),
        duration = as.numeric(difftime(Sys.time(), start_time, units = "secs"))
      )
    })
  }

  # Execute in parallel
  if (progress) {
    progressr::with_progress({
      p <- progressr::progressor(steps = nrow(experiments))
      api_call_results_list <- future.apply::future_lapply(
        seq_len(nrow(experiments)),
        function(k) {
          res <- par_worker(k)
          p()
          res
        },
        future.seed = TRUE,
        future.packages = "LLMR",
        future.globals = TRUE
      )
    })
  } else {
    api_call_results_list <- future.apply::future_lapply(
      seq_len(nrow(experiments)),
      par_worker,
      future.seed = TRUE,
      future.packages = "LLMR",
      future.globals = TRUE
    )
  }

  # Convert results to dataframe
  api_results_df <- dplyr::bind_rows(api_call_results_list)

  # Prepare output with all original columns plus results
  output_df <- experiments
  output_df$response_text <- NA_character_
  output_df$raw_response_json <- NA_character_
  output_df$success <- NA
  output_df$error_message <- NA_character_
  output_df$duration   <- NA_real_


  # Fill in results by row index
  output_df$response_text[api_results_df$row_index] <- as.character(api_results_df$response_text)
  output_df$raw_response_json[api_results_df$row_index] <- as.character(api_results_df$raw_response_json)
  output_df$success[api_results_df$row_index] <- as.logical(api_results_df$success)
  output_df$error_message[api_results_df$row_index] <- as.character(api_results_df$error_message)
  output_df$duration  [api_results_df$row_index] <- as.numeric(api_results_df$duration)

  if (verbose) {
    successful_calls <- sum(output_df$success, na.rm = TRUE)
    message(sprintf("Parallel processing completed: %d/%d experiments successful",
                    successful_calls, nrow(output_df)))
  }

  if (simplify) {
     output_df <- .unnest_config_to_cols(output_df, config_col = "config")
  }

  return(output_df)
}

#' Build Factorial Experiment Design
#'
#' Creates a tibble of experiments for factorial designs where you want to test
#' all combinations of configs, messages, and repetitions with automatic metadata.
#'
#' @param configs List of llm_config objects to test.
#' @param messages List of message lists to test (each element is a message list for one condition).
#' @param repetitions Integer. Number of repetitions per combination. Default is 1.
#' @param config_labels Character vector of labels for configs. If NULL, uses "provider_model".
#' @param message_labels Character vector of labels for message sets. If NULL, uses "messages_1", etc.
#'
#' @return A tibble with columns: config (list-column), messages (list-column),
#'   config_label, message_label, and repetition. Ready for use with call_llm_par().
#' @export
#'
#' @examples
#' \dontrun{
#'   # Factorial design: 3 configs x 2 message conditions x 10 reps = 60 experiments
#'   configs <- list(gpt4_config, claude_config, llama_config)
#'   messages <- list("Control prompt", "Treatment prompt")
#'
#'   experiments <- build_factorial_experiments(
#'     configs = configs,
#'     messages = messages,
#'     repetitions = 10,
#'     config_labels = c("gpt4", "claude", "llama"),
#'     message_labels = c("control", "treatment")
#'   )
#'
#'   # Use with call_llm_par
#'   results <- call_llm_par(experiments, progress = TRUE)
#' }
build_factorial_experiments <- function(configs,
                                        messages,
                                        repetitions = 1,
                                        config_labels = NULL,
                                        message_labels = NULL) {

  if (!requireNamespace("tibble", quietly = TRUE)) {
    stop("The 'tibble' package is required. Please install it with: install.packages('tibble')")
  }
  if (!requireNamespace("tidyr", quietly = TRUE)) {
    stop("The 'tidyr' package is required for expand_grid. Please install it with: install.packages('tidyr')")
  }
  if (!requireNamespace("dplyr", quietly = TRUE)) {
    stop("The 'dplyr' package is required for joins. Please install it with: install.packages('dplyr')")
  }

  # Validate inputs
  if (length(configs) == 0 || length(messages) == 0) {
    stop("Both configs and messages must have at least one element")
  }

  # Create config labels if not provided
  if (is.null(config_labels)) {
    config_labels <- sapply(configs, function(cfg) {
      paste(cfg$provider %||% "NA", cfg$model %||% "NA", sep = "_")
    })
  } else if (length(config_labels) != length(configs)) {
    stop("config_labels must have the same length as configs")
  }

  # Create message labels if not provided
  if (is.null(message_labels)) {
    message_labels <- paste0("messages_", seq_along(messages))
  } else if (length(message_labels) != length(messages)) {
    stop("message_labels must have the same length as messages")
  }

  # Create lookup tables
  configs_df <- tibble::tibble(
    config_idx = seq_along(configs),
    config = configs,
    config_label = config_labels
  )

  messages_df <- tibble::tibble(
    message_idx = seq_along(messages),
    messages = messages,
    message_label = message_labels
  )

  # Create factorial design
  experiments <- tidyr::expand_grid(
    config_idx = configs_df$config_idx,
    message_idx = messages_df$message_idx,
    repetition = seq_len(repetitions)
  ) |>
    dplyr::left_join(configs_df, by = "config_idx") |>
    dplyr::left_join(messages_df, by = "message_idx") |>
    dplyr::select(config, messages, config_label, message_label, repetition)

  message(sprintf("Built %d experiments: %d configs x %d message sets x %d repetitions",
                  nrow(experiments), length(configs), length(messages), repetitions))

  return(experiments)
}

#' Setup Parallel Environment for LLM Processing
#'
#' Convenience function to set up the future plan for optimal LLM parallel processing.
#' Automatically detects system capabilities and sets appropriate defaults.
#'
#' @param strategy Character. The future strategy to use. Options: "multisession", "multicore", "sequential".
#'                If NULL (default), automatically chooses "multisession".
#' @param workers Integer. Number of workers to use. If NULL, auto-detects optimal number
#'                (availableCores - 1, capped at 8).
#' @param verbose Logical. If TRUE, prints setup information.
#'
#' @return Invisibly returns the previous future plan.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Automatic setup
#'   old_plan <- setup_llm_parallel()
#'
#'   # Manual setup with specific workers
#'   setup_llm_parallel(workers = 4, verbose = TRUE)
#'
#'   # Force sequential processing for debugging
#'   setup_llm_parallel(strategy = "sequential")
#'
#'   # Restore old plan if needed
#'   future::plan(old_plan)
#' }
setup_llm_parallel <- function(strategy = NULL, workers = NULL, verbose = FALSE) {

  if (!requireNamespace("future", quietly = TRUE)) {
    stop("The 'future' package is required. Please install it with: install.packages('future')")
  }

  current_plan <- future::plan()
  strategy <- strategy %||% "multisession"

  if (is.null(workers)) {
    available_cores <- future::availableCores()
    workers <- max(1, available_cores - 1)
    workers <- min(workers, 8) # Cap at reasonable maximum for API calls
  } else {
    workers <- max(1, as.integer(workers))
  }

  if (verbose) {
    message(sprintf("Setting up parallel environment:"))
    message(sprintf("  Requested Strategy: %s", strategy))
    message(sprintf("  Requested Workers: %d", workers))
    message(sprintf("  Available cores on system: %d", future::availableCores()))
  }

  if (strategy == "sequential") {
    future::plan(future::sequential)
  } else if (strategy == "multicore") {
    if (.Platform$OS.type == "windows") {
      warning("'multicore' is not supported on Windows. Using 'multisession' instead.")
      future::plan(future::multisession, workers = workers)
    } else {
      future::plan(future::multicore, workers = workers)
    }
  } else if (strategy == "multisession") {
    future::plan(future::multisession, workers = workers)
  } else {
    stop("Invalid strategy. Choose from: 'sequential', 'multicore', 'multisession'")
  }

  if (verbose) {
    message(sprintf("Parallel environment set to: %s with %d workers.",
                    class(future::plan())[1], future::nbrOfWorkers()))
  }

  invisible(current_plan)
}

#' Reset Parallel Environment
#'
#' Resets the future plan to sequential processing.
#'
#' @param verbose Logical. If TRUE, prints reset information.
#'
#' @return Invisibly returns the future plan that was in place before resetting to sequential.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Setup parallel processing
#'   old_plan <- setup_llm_parallel(workers = 2)
#'
#'   # Do some parallel work...
#'
#'   # Reset to sequential
#'   reset_llm_parallel(verbose = TRUE)
#'
#'   # Optionally restore the specific old_plan if it was non-sequential
#'   # future::plan(old_plan)
#' }
reset_llm_parallel <- function(verbose = FALSE) {

  if (!requireNamespace("future", quietly = TRUE)) {
    warning("The 'future' package is not available. Cannot reset plan.")
    return(invisible(NULL))
  }

  if (verbose) {
    message("Resetting parallel environment to sequential processing...")
  }

  previous_plan <- future::plan(future::sequential)

  if (verbose) {
    message("Parallel environment reset complete. Previous plan was: ", class(previous_plan)[1])
  }

  invisible(previous_plan)
}
# param_alias.R  -------------------------------------------------------------
# Canonical-to-provider parameter translation (inspired by LangChain)
#   â€“ canonical names follow the OpenAI spelling
#   â€“ unknown keys are forwarded untouched for maximal future-proofing
#
# Supported canonical names:
#   temperature, max_tokens, top_p, top_k,
#   frequency_penalty, presence_penalty, repetition_penalty,
#   thinking_budget, include_thoughts
#
##############################################################################

.translate_params <- function(provider, mp = list()) {

  ## --- 1. canonical ---> provider field names ---------------------------------
  map <- switch(
    provider,
    gemini = c(
      max_tokens      = "maxOutputTokens",
      top_p           = "topP",
      top_k           = "topK",
      thinking_budget = "thinkingBudget",
      include_thoughts= "includeThoughts"
    ),
    anthropic = c(
      thinking_budget = "budget_tokens",    # lives in $thinking
      include_thoughts= "include_thoughts"  # handled later
    ),
    openai   = character(),                 # identical
    groq     = character(),                 # identical
    together = character(),                 # identical
    deepseek = character(),                 # identical
    voyage   = character(),                 # identical
    character()                             # default: pass-through
  )

  if (length(map)) {
    renames <- intersect(names(mp), names(map))
    names(mp)[match(renames, names(mp))] <- map[renames]
  }

  ## --- 2. warn on obviously unsupported knobs ------------------------------
  unsupported <- switch(
    provider,
    gemini    = c("frequency_penalty", "presence_penalty", "repetition_penalty"),
    anthropic = c("top_k", "frequency_penalty", "presence_penalty",
                  "repetition_penalty"),
    character()
  )
  bad <- intersect(names(mp), unsupported)
  if (length(bad))
    warning(
      sprintf("Parameters not recognised by %s API dropped: %s",
              provider, paste(bad, collapse = ", "))
    )

  mp[setdiff(names(mp), unsupported)]
}
# LLM_robust_utils.R
# -------------------------------------------------------------------
# This file provides additional features that improve reliability and
# fault tolerance of calls made through the LLMR package:
#
#   1. retry_with_backoff() - A generic utility that retries any function
#      with exponential backoff (INTERNAL, not exported).
#
#   2. call_llm_robust() - A specialized wrapper around call_llm() with
#      simple retry logic for rate-limit errors (HTTP 429), now
#      enhanced with variable backoff and optional memoization.
#
#   3. cache_llm_call() - A caching wrapper for call_llm(), preventing
#      repeated identical requests.
#
#   4. log_llm_error() - A convenience function for logging errors
#      with timestamps.
#
# -------------------------------------------------------------------
# 1. Exponential Backoff (Internal)
# -------------------------------------------------------------------

# A generic utility that calls a function repeatedly with exponentially
# increasing wait time after each failure. Not exported, no roxygen docs.

retry_with_backoff <- function(func,
                               tries = 5,
                               initial_wait = 10,
                               backoff_factor = 5,
                               error_filter_func = NULL,
                               ...) {
  wait_time <- initial_wait
  for (attempt in seq_len(tries)) {
    result <- tryCatch(
      func(...),
      error = function(e) {
        if (!is.null(error_filter_func) && !error_filter_func(e)) {
          stop(e)
        }
        message(sprintf("Error on attempt %d: %s", attempt, conditionMessage(e)))
        message(sprintf("Waiting %d seconds before retry...", wait_time))
        Sys.sleep(wait_time)
        wait_time <- wait_time * backoff_factor
        return(NULL)
      }
    )
    if (!is.null(result)) {
      return(result)
    }
  }
  stop("All attempts failed.")
}

# -------------------------------------------------------------------
# 2. Rate-Limit-Aware LLM API Call
# -------------------------------------------------------------------

#' Robustly Call LLM API (Simple Retry)
#'
#' Wraps \code{\link{call_llm}} to handle rate-limit errors (HTTP 429 or related
#' "Too Many Requests" messages). It retries the call a specified number of times,
#' using exponential backoff. You can also choose to cache responses if you do
#' not need fresh results each time.
#'
#' @param config An \code{llm_config} object from \code{\link{llm_config}}.
#' @param messages A list of message objects (or character vector for embeddings).
#' @param tries Integer. Number of retries before giving up. Default is 5.
#' @param wait_seconds Numeric. Initial wait time (seconds) before the first retry. Default is 10.
#' @param backoff_factor Numeric. Multiplier for wait time after each failure. Default is 5.
#' @param verbose Logical. If TRUE, prints the full API response.
#' @param json Logical. If TRUE, returns the raw JSON as an attribute.
#' @param memoize Logical. If TRUE, calls are cached to avoid repeated identical requests. Default is FALSE.
#'
#' @return The successful result from \code{\link{call_llm}}, or an error if all retries fail.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Basic usage:
#'   robust_resp <- call_llm_robust(
#'     config = my_llm_config,
#'     messages = list(list(role = "user", content = "Hello, LLM!")),
#'     tries = 5,
#'     wait_seconds = 10,
#'     memoize = FALSE
#'   )
#'   cat("Response:", robust_resp, "\n")
#' }
call_llm_robust <- function(config, messages,
                            tries = 5,
                            wait_seconds = 10,
                            backoff_factor = 5,
                            verbose = FALSE,
                            json = FALSE,
                            memoize = FALSE) {

  # Internal helper that calls either the direct function or the cached variant
  call_func <- function() {
    if (memoize) {
      if (!requireNamespace("memoise", quietly = TRUE)) {
        stop("memoize=TRUE requires the 'memoise' package. Please install it (install.packages('memoise')).")
      }
      return(cache_llm_call(config, messages, verbose = verbose, json = json))
    } else {
      return(call_llm(config, messages, verbose = verbose, json = json))
    }
  }

  # Error filter for retry_with_backoff: only retry for specific conditions
  is_retryable_error <- function(e) {
    err_msg <- conditionMessage(e)
    # Simple detection of rate-limit-like errors
    is_rate_error <- grepl("429|rate limit|too many requests|exceeded",
                           err_msg, ignore.case = TRUE)
    if (is_rate_error) {
      log_llm_error(e) # Log the rate error specifically if we are about to retry it
      return(TRUE) # Retry this error
    }
    return(FALSE) # Do not retry other errors
  }
  
  # Original error handling logic for non-retryable errors or after all retries fail
  tryCatch(
    retry_with_backoff(
      func = call_func,
      tries = tries,
      initial_wait = wait_seconds,
      backoff_factor = backoff_factor,
      error_filter_func = is_retryable_error
    ),
    error = function(e) {
      if (!is_retryable_error(e)) {
         log_llm_error(e)
      }
      stop(e)
    }
  )
}

# -------------------------------------------------------------------
# 3. Caching Wrapper
# -------------------------------------------------------------------

#' Cache LLM API Calls
#'
#' A memoised version of \code{\link{call_llm}} to avoid repeated identical requests.
#'
#' @param config An \code{llm_config} object from \code{\link{llm_config}}.
#' @param messages A list of message objects or character vector for embeddings.
#' @param verbose Logical. If TRUE, prints the full API response (passed to \code{\link{call_llm}}).
#' @param json Logical. If TRUE, returns raw JSON (passed to \code{\link{call_llm}}).
#'
#' @details
#' - Requires the \code{memoise} package. Add \code{memoise} to your
#'   package's DESCRIPTION.
#' - Clearing the cache can be done via \code{memoise::forget(cache_llm_call)}
#'   or by restarting your R session.
#'
#' @return The (memoised) response object from \code{\link{call_llm}}.
#'
#' @importFrom memoise memoise
#' @export
#' @name cache_llm_call
#'
#' @examples
#' \dontrun{
#'   # Using cache_llm_call:
#'   response1 <- cache_llm_call(my_config, list(list(role="user", content="Hello!")))
#'   # Subsequent identical calls won't hit the API unless we clear the cache.
#'   response2 <- cache_llm_call(my_config, list(list(role="user", content="Hello!")))
#' }
cache_llm_call <- memoise::memoise(function(config, messages, verbose = FALSE, json = FALSE) {
  if (!requireNamespace("memoise", quietly = TRUE)) {
    stop("Caching with cache_llm_call requires the 'memoise' package. Please install it (install.packages('memoise')).")
  }
  call_llm(config, messages, verbose = verbose, json = json)
})

# -------------------------------------------------------------------
# 4. Error Logging
# -------------------------------------------------------------------

#' Log LLMR Errors
#'
#' Logs an error with a timestamp for troubleshooting.
#'
#' @param err An error object.
#'
#' @return Invisibly returns \code{NULL}.
#' @export
#'
#' @examples
#' \dontrun{
#'   # Example of logging an error by catching a failure:
#'   # Use a deliberately fake API key to force an error
#'   config_test <- llm_config(
#'     provider = "openai",
#'     model = "gpt-3.5-turbo",
#'     api_key = "FAKE_KEY",
#'     temperature = 0.5,
#'     top_p = 1,
#'     max_tokens = 30
#'   )
#'
#'   tryCatch(
#'     call_llm(config_test, list(list(role = "user", content = "Hello world!"))),
#'     error = function(e) log_llm_error(e)
#'   )
#' }
log_llm_error <- function(err) {
  stamp <- Sys.time()
  msg <- conditionMessage(err)
  message(sprintf("[%s] LLMR Error: %s", stamp, msg))
  invisible(NULL)
}









# LLMR_tidy.R ---------------------------------------------------------------
#' Vectorised LLM transformer
#'
#' @importFrom tidyr expand_grid
#' @importFrom rlang `:=`
#'
#' \code{llm_fn()} turns every element (or row) of \code{x} into an LLM prompt
#' (via glue templating) and returns the modelâ€™s reply.
#'
#' *Stateless* â€“ no memory across calls, so it is **not** an agent.
#'
#' @param x  A character vector **or** a data.frame/tibble.
#' @param prompt A glue template string.
#'   *If* \code{x} is a data frame, use \code{{col}} placeholders;
#'   *if* \code{x} is a vector, refer to the element as \code{{x}}.
#' @param .config An \link{llm_config} object.
#' @param .system_prompt Optional system message (character scalar).
#' @param ... Passed unchanged to \link{call_llm_broadcast} (e.g.\ \code{tries},
#'   \code{progress}, \code{verbose}).
#'
#' @return A character vector the same length as \code{x}.
#'   Failed calls yield \code{NA}.
#' @details
#' Runs each prompt through `call_llm_broadcast()`, which forwards the
#' requests to `call_llm_par()`.
#' Internally each prompt is passed as a
#' **plain character vector** (or a
#' named character vector when `.system_prompt` is supplied).
#' That core engine executes them *in parallel* according
#' to the current *future* plan.
#' For instant multi-core use, call `setup_llm_parallel(workers = 4)` (or whatever
#' number you prefer) once per session; revert with `reset_llm_parallel()`.
#'
#' @seealso setup_llm_parallel, reset_llm_parallel, call_llm_par
#' @export
#'
#' @examples
#' ## --- Vector input ------------------------------------------------------
#' \dontrun{
#' cfg <- llm_config(
#'   provider = "openai",
#'   model    = "gpt-4.1-nano",
#'   api_key  =  Sys.getenv("OPENAI_API_KEY"),
#'   temperature = 0
#' )
#'
#' words <- c("excellent", "awful", "average")
#'
#' llm_fn(
#'   words,
#'   prompt   = "Classify sentiment of '{x}' as Positive, Negative, or Neutral.",
#'   .config  = cfg,
#'   .system_prompt = "Respond with ONE word only."
#' )
#'
#' ## --- Data-frame input inside a tidyverse pipeline ----------------------
#' library(dplyr)
#'
#' reviews <- tibble::tibble(
#'   id     = 1:3,
#'   review = c("Great toaster!", "Burns bread.", "It's okay.")
#' )
#'
#' reviews |>
#'   llm_mutate(
#'     sentiment,
#'     prompt  = "Classify the sentiment of this review: {review}",
#'     .config = cfg,
#'     .system_prompt = "Respond with Positive, Negative, or Neutral."
#'   )
#' }
llm_fn <- function(x,
                   prompt,
                   .config,
                   .system_prompt = NULL,
                   ...) {

  stopifnot(inherits(.config, "llm_config"))

  user_txt <- if (is.data.frame(x)) {
    glue::glue_data(x, prompt)
  } else {
    glue::glue_data(list(x = x), prompt)
  }

  msgs <- lapply(user_txt, function(txt) {
    if (is.null(.system_prompt)) {
      txt                                   # single-turn chat
    } else {
      c(system = .system_prompt, user = txt)  # named vector: system then user
    }
  })

  res <- call_llm_broadcast(
    config        = .config,
    messages_list = msgs,
    ...
  )

  ifelse(res$success, res$response_text, NA_character_)
}

#' Mutate a data frame with LLM output
#'
#' A convenience wrapper around \link{llm_fn} that inserts the result as a new
#' column via \link[dplyr]{mutate}.
#'
#' @inheritParams llm_fn
#' @param .data  A data frame / tibble.
#' @param output Unquoted name of the new column you want to add.
#' @param .before,.after Standard \link[dplyr]{mutate} column-placement helpers.
#' @details
#' Internally calls `llm_fn()`, so the API requests inherit the same
#' parallel behaviour.  Activate parallelism with
#' `setup_llm_parallel()` and shut it off with `reset_llm_parallel()`.
#'
#' @seealso llm_fn, setup_llm_parallel, reset_llm_parallel
#' @export
#'
#' @examples
#' ## See examples under \link{llm_fn}.
llm_mutate <- function(.data,
                       output,
                       prompt,
                       .config,
                       .system_prompt = NULL,
                       .before = NULL,
                       .after  = NULL,
                       ...) {

  out <- rlang::enquo(output)
  new_vals <- llm_fn(.data,
                     prompt         = prompt,
                     .config        = .config,
                     .system_prompt = .system_prompt,
                     ...)
  .data |>
    dplyr::mutate(
      !!out := new_vals,
      .before = {{ .before }},
      .after  = {{ .after }}
    )
}

utils::globalVariables(
  c("config", "messages", "config_label",
    "message_label", "repetition",
    ".param_name_sweep", ".param_value_sweep")
)
# LLMR.R
# -------------------------------------------------------------------
# This file provides the core functionality for the LLMR package,
# including configuration, API dispatching, and response parsing.
# It defines the main S3 generic `call_llm()` and provides specific
# implementations for various providers like OpenAI, Anthropic, Gemini, etc.
#
# Key Features:
#   1. llm_config() - Standardized configuration object.
#   2. call_llm() - S3 generic for dispatching to the correct provider API.
#   3. Provider-specific implementations (e.g., call_llm.openai).
#   4. Support for both generative and embedding models.
#   5. (New) Support for multimodal inputs (text and files) for capable providers.
# -------------------------------------------------------------------

# ----- Internal Helper Functions -----

#' Normalise message inputs
#'
#' - character vector           ---> each element becomes a `"user"` message
#' - **named** character vector ---> the names are taken as `role`s
#' - already-well-formed list    ---> returned untouched
#' Called once in `call_llm()` (only when **not** in embedding mode).
#' @keywords internal
#' @noRd
.normalize_messages <- function(messages) {
  # 1. leave proper message objects unchanged
  if (is.list(messages) &&
      length(messages) > 0 &&
      is.list(messages[[1]]) &&
      !is.null(messages[[1]]$role) &&
      !is.null(messages[[1]]$content)) {
    return(messages)
  }

  # 2. named character ---> role = names(messages)
  if (is.character(messages) && !is.null(names(messages))) {
    return(unname(
      purrr::imap(messages, \(txt, role)
                  list(role = role, content = txt)
      )
    ))
  }



  # 3. bare character ---> assume user
  if (is.character(messages)) {
    return(lapply(messages, \(txt) list(role = "user", content = txt)))
  }

  stop("`messages` must be a character vector or a list of message objects.")
}

#' Process a file for multimodal API calls
#'
#' Reads a file, determines its MIME type, and base64 encodes it.
#' This is an internal helper function.
#' @param file_path The path to the file.
#' @return A list containing the mime_type and base64_data.
#' @keywords internal
#' @noRd
#' @importFrom mime guess_type
#' @importFrom base64enc base64encode
.process_file_content <- function(file_path) {
  if (!file.exists(file_path)) {
    stop("File not found at path: ", file_path)
  }
  # Guess MIME type from file extension
  mime_type <- mime::guess_type(file_path, empty = "application/octet-stream")

  # Read file and encode using the reliable base64enc package
  base64_data <- base64enc::base64encode(what = file_path)

  return(list(
    mime_type = mime_type,
    base64_data = base64_data
  ))
}

## keep only NULL-free elements -----
## this makes sure innocent api calls (what the user doesn't explicitly mention
## is not mentioned in the api call)
.drop_null <- function(x) purrr::compact(x)


#' Perform API Request
#'
#' Internal helper function to perform the API request and process the response.
#'
#' @keywords internal
#' @noRd
#' @importFrom httr2 req_perform resp_body_raw resp_body_json
perform_request <- function(req, verbose, json) {
  resp <- httr2::req_perform(req)

  if (httr2::resp_status(resp) >= 400) {
    stop(rawToChar(httr2::resp_body_raw(resp)), call. = FALSE)
  }

  # Get the raw response as text
  raw_response <- httr2::resp_body_raw(resp)
  raw_json <- rawToChar(raw_response)
  # Parse the response as JSON
  content <- httr2::resp_body_json(resp)

  if (verbose) {
    cat("Full API Response:\n")
    print(content)
  }

  text <- extract_text(content)
  attr(text, "full_response") <- content
  attr(text, "raw_json") <- raw_json

  if (json) {
    return(text)
  }

  # By default, for non-json output, just return the text
  return(as.character(text))
}

#' Extract Text from API Response
#'
#' Internal helper function to extract text from the API response content.
#'
#' @keywords internal
#' @noRd
extract_text <- function(content) {
    # Handle embeddings FIRST with more flexible logic
    if (is.list(content) && (!is.null(content$data) || !is.null(content$embedding))) {
      return(content)
    }

    if (!is.null(content$choices)) {
      # For APIs like OpenAI, Groq, Together AI
      if (length(content$choices) == 0 || is.null(content$choices[[1]]$message$content)) {
        return(NA_character_)
      }
      return(content$choices[[1]]$message$content)
    }

    if (!is.null(content$content)) {
      # For Anthropic
      if (length(content$content) == 0 || is.null(content$content[[1]]$text)) {
        return(NA_character_)
      }
      return(content$content[[1]]$text)
    }

    if (!is.null(content$candidates)) {
      # For Gemini API
      if (length(content$candidates) == 0 ||
          is.null(content$candidates[[1]]$content$parts) ||
          length(content$candidates[[1]]$content$parts) == 0 ||
          is.null(content$candidates[[1]]$content$parts[[1]]$text)) {
        return(NA_character_)
      }
      return(content$candidates[[1]]$content$parts[[1]]$text)
    }

    # Fallback - return content as-is instead of throwing error
    return(content)
}

#' Format Anthropic Messages
#'
#' Internal helper function to format messages for Anthropic API.
#' This helper is now simplified as logic is moved into call_llm.anthropic
#'
#' @keywords internal
#' @noRd
format_anthropic_messages <- function(messages) {
  system_messages <- purrr::keep(messages, ~ .x$role == "system")
  user_messages <- purrr::keep(messages, ~ .x$role != "system")

  system_text <- if (length(system_messages) > 0) {
    paste(sapply(system_messages, function(x) x$content), collapse = " ")
  } else {
    NULL
  }

  # The complex formatting is now handled directly in call_llm.anthropic
  # to support multimodal content. This function just separates system/user messages.
  list(system_text = system_text, user_messages = user_messages)
}

# Helper to determine the endpoint
get_endpoint <- function(config, default_endpoint) {
  if (!is.null(config$model_params$api_url)) {
    return(config$model_params$api_url)
  }
  default_endpoint
}

# ----- Exported Functions -----

#' Create LLM Configuration
#'
#' @param provider Provider name (openai, anthropic, groq, together, voyage, gemini, deepseek)
#' @param model Model name to use
#' @param api_key API key for authentication
#' @param troubleshooting Prints out all api calls. USE WITH EXTREME CAUTION as it prints your API key.
#' @param base_url Optional base URL override
#' @param embedding Logical indicating embedding mode: NULL (default, uses prior defaults), TRUE (force embeddings), FALSE (force generative)
#' @param ... Additional provider-specific parameters
#' @return Configuration object for use with call_llm()
#' @export
#' @examples
#' \dontrun{
#'   cfg <- llm_config(
#'     provider   = "openai",
#'     model      = "gpt-4o-mini",
#'     api_key    = Sys.getenv("OPENAI_API_KEY"),
#'     temperature = 0.7,
#'     max_tokens  = 500)
#'
#'   call_llm(cfg, "Hello!")  # one-shot, bare string
#' }
llm_config <- function(provider, model, api_key, troubleshooting = FALSE, base_url = NULL, embedding = NULL, ...) {
  model_params <- list(...)
  # Handle base_url passed via ... for backward compatibility, renaming to api_url internally
  if (!is.null(base_url)) {
    model_params$api_url <- base_url
  }
  config <- list(
    provider = provider,
    model = model,
    api_key = api_key,
    troubleshooting = troubleshooting,
    embedding = embedding,
    model_params = model_params
  )
  class(config) <- c("llm_config", provider)
  return(config)
}

#' Call LLM API
#'
#' Sends a message to the specified LLM API and retrieves the response.
#'
#' @param config An `llm_config` object created by `llm_config()`.
#' @param messages Either
#'   \itemize{
#'     \item a bare character vector (each element becomes a `"user"` message);
#'     \item a **named** character vector whose names are taken as `role`s,
#'           e.g.\ \code{c(system = "Be concise.", user = "Hi")};
#'     \item the classical list-of-lists with explicit \code{role}/\code{content}.
#'   }
#'   For multimodal requests the \code{content} field of a message can itself be
#'   a list of parts, e.g.\ \code{list(type = "file", path = "image.png")}.
#' @param verbose Logical. If `TRUE`, prints the full API response.
#' @param json Logical. If `TRUE`, the returned text will have the raw JSON response
#'   and the parsed list as attributes.
#'
#' @return The generated text response or embedding results. If `json=TRUE`,
#'   attributes `raw_json` and `full_response` are attached.
#' @export
#' @examples
#' \dontrun{
#'   cfg <- llm_config("openai", "gpt-4o-mini", Sys.getenv("OPENAI_API_KEY"))
#'
#'   # 1. Bare string
#'   call_llm(cfg, "What is prompt engineering?")
#'
#'   # 2. Named character vector (quick system + user)
#'   call_llm(cfg, c(system = "Be brief.", user = "Summarise the Kuznets curve."))
#'
#'   # 3. Classic list form (still works)
#'   call_llm(cfg, list(list(role = "user", content = "Hello!")))
#'
#'   # 4. Multimodal (vision-capable model required)
#'   multi_cfg <- llm_config("openai", "gpt-4o", Sys.getenv("OPENAI_API_KEY"))
#'   msg <- list(list(role = "user", content = list(
#'            list(type = "text", text = "Describe this picture"),
#'            list(type = "file", path = "path/to/image.png"))))
#'   call_llm(multi_cfg, msg)
#' }
call_llm <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (config$troubleshooting == TRUE){
    print("\n\n Inside call_llm for troubleshooting\n")
    print("\n****\nBE CAREFUL THIS BIT CONTAINS YOUR API KEY! DO NOT REPORT IT AS IS!\n****\n")
    print(messages)
    print(config)
    print("\n\n")
  }

  UseMethod("call_llm", config)
}

#' @export
call_llm.default <- function(config, messages, verbose = FALSE, json = FALSE) {
  # This default is mapped to the OpenAI-compatible endpoint structure
  message("Provider-specific function not present, defaulting to OpenAI format.")
  call_llm.openai(config, messages, verbose, json)
}

#' @export
call_llm.openai <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding)) {
    return(call_llm.openai_embedding(config, messages, verbose, json))
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = "https://api.openai.com/v1/chat/completions")

  # Format messages with multimodal support
  formatted_messages <- lapply(messages, function(msg) {
    if (msg$role != "user" || is.character(msg$content)) {
      return(msg)
    }

    if (is.list(msg$content)) {
      content_parts <- lapply(msg$content, function(part) {
        if (part$type == "text") {
          return(list(type = "text", text = part$text))
        } else if (part$type == "file") {
          file_data <- .process_file_content(part$path)
          data_uri <- paste0("data:", file_data$mime_type, ";base64,", file_data$base64_data)
          return(list(type = "image_url", image_url = list(url = data_uri)))
        } else {
          return(NULL)
        }
      })
      msg$content <- purrr::compact(content_parts)
    }
    return(msg)
  })

  body <- .drop_null(list(
    model             = config$model,
    messages          = formatted_messages,
    temperature       = config$model_params$temperature,
    max_tokens        = config$model_params$max_tokens,
    top_p             = config$model_params$top_p,
    frequency_penalty = config$model_params$frequency_penalty,
    presence_penalty  = config$model_params$presence_penalty
  ))

  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", config$api_key)
    ) |>
    httr2::req_body_json(body)

  perform_request(req, verbose, json)
}

#' @export
call_llm.anthropic <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding)) {
    stop("Embedding models are not currently supported for Anthropic!")
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = "https://api.anthropic.com/v1/messages")

  use_thinking_beta <- !is.null(config$model_params$thinking_budget) ||
    isTRUE(config$model_params$include_thoughts)

  # Use the helper to separate system messages
  formatted <- format_anthropic_messages(messages)

  # Process user messages for multimodal content
  processed_user_messages <- lapply(formatted$user_messages, function(msg) {
    # â”€â”€ KEEP STRING CONTENT AS-IS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (is.character(msg$content)) {
      return(list(role = msg$role, content = msg$content))
    }

    # â”€â”€ OTHERWISE (images / tools) BUILD BLOCKS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    content_blocks <- lapply(msg$content, function(part) {
      if (part$type == "text")
        list(type = "text", text = part$text)
      else if (part$type == "file") {
        fd <- .process_file_content(part$path)
        list(type = "image",
             source = list(type = "base64",
                           media_type = fd$mime_type,
                           data = fd$base64_data))
      } else NULL
    })
    list(role = msg$role, content = content_blocks |> purrr::compact())
  })

  ### translate & pull out Anthropic-specific aliases
  params <- .translate_params("anthropic", config$model_params)

  if (is.null(params$max_tokens))
    warning('Anthropic requires max_tokens; setting it at 2048.')



  body <- .drop_null(list(
    model      = config$model,
    max_tokens = params$max_tokens %||% 2048,
    temperature= params$temperature,
    top_p      = params$top_p,
    messages   = processed_user_messages,
    thinking   = if (!is.null(params$thinking_budget) &&
                     !is.null(params$include_thoughts))
      list(
        type          = "enabled",
        budget_tokens = params$thinking_budget
      )
  ))

  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type"      = "application/json",
      "x-api-key"         = config$api_key,
      "anthropic-version" = "2023-06-01",
      !!! (
        if (!is.null(body$thinking))
          list("anthropic-beta" = "extended-thinking-2025-05-14")
      )
    ) |>
    httr2::req_body_json(body)

  perform_request(req, verbose, json)
}



#' @export
call_llm.gemini <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding) || grepl("embedding", config$model, ignore.case = TRUE)) {
    return(call_llm.gemini_embedding(config, messages, verbose, json))
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = paste0("https://generativelanguage.googleapis.com/v1beta/models/", config$model, ":generateContent"))

  ## convert canonical names ---> Gemini native
  params <- .translate_params("gemini", config$model_params)

  system_messages <- purrr::keep(messages, ~ .x$role == "system")
  other_messages <- purrr::keep(messages, ~ .x$role != "system")
  system_instruction <- if (length(system_messages) > 0) {
    list(parts = list(list(text = paste(sapply(system_messages, function(x) x$content), collapse = " "))))
  } else {
    NULL
  }

  formatted_messages <- lapply(other_messages, function(msg) {
    role <- if (msg$role == "assistant") "model" else "user"
    content_parts <- list()
    if (is.character(msg$content)) {
      content_parts <- list(list(text = msg$content))
    } else if (is.list(msg$content)) {
      content_parts <- lapply(msg$content, function(part) {
        if (part$type == "text") {
          return(list(text = part$text))
        } else if (part$type == "file") {
          file_data <- .process_file_content(part$path)
          return(list(inlineData = list(mimeType = file_data$mime_type, data = file_data$base64_data)))
        } else {
          return(NULL)
        }
      })
      content_parts <- purrr::compact(content_parts)
    }
    list(role = role, parts = content_parts)
  })

  body <- .drop_null(list(
    contents = formatted_messages,
    generationConfig = .drop_null(list(
      temperature     = params$temperature,
      maxOutputTokens = params$maxOutputTokens,
      topP            = params$topP,
      topK            = params$topK
    )),
    thinkingConfig = if (!is.null(params$thinkingBudget) ||
                         !is.null(params$includeThoughts))
      .drop_null(list(
        budgetTokens   = params$thinkingBudget,
        includeThoughts= isTRUE(params$includeThoughts)))
  ))


  if (!is.null(system_instruction))
    body$systemInstruction <- system_instruction

  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type" = "application/json",
      "x-goog-api-key" = config$api_key
    ) |>
    httr2::req_body_json(body)

  perform_request(req, verbose, json)
}


# ----- Unmodified Provider Functions (for non-vision tasks) -----

#' @export
call_llm.groq <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding)) {
    stop("Embedding models are not currently supported for Groq!")
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = "https://api.groq.com/openai/v1/chat/completions")

  body <- .drop_null(list(
    model      = config$model,
    messages   = messages,
    temperature= config$model_params$temperature,
    max_tokens = config$model_params$max_tokens
  ))


  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", config$api_key)
    ) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json)
}

#' @export
call_llm.together <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding)) {
    return(call_llm.together_embedding(config, messages, verbose, json))
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = "https://api.together.xyz/v1/chat/completions")

  body <- .drop_null(list(
    model              = config$model,
    messages           = messages,
    max_tokens         = config$model_params$max_tokens,
    temperature        = config$model_params$temperature,
    top_p              = config$model_params$top_p,
    top_k              = config$model_params$top_k,
    repetition_penalty = config$model_params$repetition_penalty
  ))

  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", config$api_key)
    ) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json)
}

#' @export
call_llm.deepseek <- function(config, messages, verbose = FALSE, json = FALSE) {
  if (isTRUE(config$embedding)) {
    stop("Embedding models are not currently supported for DeepSeek!")
  }
  messages <- .normalize_messages(messages)
  endpoint <- get_endpoint(config, default_endpoint = "https://api.deepseek.com/chat/completions")

  body <- .drop_null(list(
    model      = config$model %||% "deepseek-chat",
    messages   = messages,
    temperature= config$model_params$temperature,
    max_tokens = config$model_params$max_tokens,
    top_p      = config$model_params$top_p
  ))

  req <- httr2::request(endpoint) |>
    httr2::req_headers(
      "Content-Type" = "application/json",
      "Authorization" = paste("Bearer", config$api_key)
    ) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json)
}

# ----- Embedding-specific Handlers -----

#' @export
#' @keywords internal
call_llm.openai_embedding <- function(config, messages, verbose = FALSE, json = FALSE) {
  endpoint <- get_endpoint(config, default_endpoint = "https://api.openai.com/v1/embeddings")
  texts <- if (is.character(messages)) messages else sapply(messages, `[[`, "content")
  body <- list(model = config$model, input = texts)
  req <- httr2::request(endpoint) |>
    httr2::req_headers("Content-Type" = "application/json", "Authorization" = paste("Bearer", config$api_key)) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json=TRUE)
}

#' @export
call_llm.voyage <- function(config, messages, verbose = FALSE, json = FALSE) {
  # Voyage is embeddings-only in this implementation
  return(call_llm.voyage_embedding(config, messages, verbose, json))
}

#' @export
#' @keywords internal
call_llm.voyage_embedding <- function(config, messages, verbose = FALSE, json = FALSE) {
  endpoint <- get_endpoint(config, default_endpoint = "https://api.voyageai.com/v1/embeddings")
  texts <- if (is.character(messages)) messages else sapply(messages, `[[`, "content")
  body <- list(input = texts, model = config$model)
  req <- httr2::request(endpoint) |>
    httr2::req_headers("Content-Type" = "application/json", "Authorization" = paste("Bearer", config$api_key)) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json = TRUE)
}

#' @export
#' @keywords internal
call_llm.together_embedding <- function(config, messages, verbose = FALSE, json = FALSE) {
  endpoint <- get_endpoint(config, default_endpoint = "https://api.together.xyz/v1/embeddings")
  texts <- if (is.character(messages)) messages else sapply(messages, `[[`, "content")
  body <- list(model = config$model, input = texts)
  req <- httr2::request(endpoint) |>
    httr2::req_headers("Content-Type" = "application/json", "Authorization" = paste("Bearer", config$api_key)) |>
    httr2::req_body_json(body)
  perform_request(req, verbose, json=TRUE)
}

#' @export
#' @keywords internal
call_llm.gemini_embedding <- function(config, messages, verbose = FALSE, json = FALSE) {
  # Endpoint for single content embedding
  endpoint <- paste0("https://generativelanguage.googleapis.com/v1beta/models/", config$model, ":embedContent")

  # Handle both character vectors and message lists to get texts
  texts <- if (is.character(messages)) {
    messages
  } else {
    # Assuming messages is a list of lists like list(list(role="user", content="..."))
    # or just a list of character strings if not strictly following message format.
    # This part might need adjustment if 'messages' can be complex lists.
    # For get_batched_embeddings, 'messages' will be a character vector (batch_texts).
    sapply(messages, function(msg) {
      if (is.list(msg) && !is.null(msg$content)) {
        msg$content
      } else if (is.character(msg)) {
        msg
      } else {
        as.character(msg) # Fallback
      }
    })
  }

  results <- vector("list", length(texts)) # Pre-allocate list

  for (i in seq_along(texts)) {
    current_text <- texts[i]
    body <- list(
      content = list(
        parts = list(list(text = current_text))
      )
    )

    req <- httr2::request(endpoint) |>
      httr2::req_url_query(key = config$api_key) |>
      httr2::req_headers("Content-Type" = "application/json") |>
      httr2::req_body_json(body)

    if (verbose) {
      cat("Making single embedding request to:", endpoint, "for text", i, "\n")
      # cat("Text snippet:", substr(current_text, 1, 50), "...\n") # Optional: for debugging
    }

    api_response_content <- NULL # Initialize
    tryCatch({
      resp <- httr2::req_perform(req)
      api_response_content <- httr2::resp_body_json(resp)
    }, error = function(e) {
      if (verbose) {
        message("LLMR Error during Gemini embedding for text ", i, ": ", conditionMessage(e))
      }
      # api_response_content remains NULL if error
    })

    if (!is.null(api_response_content) && !is.null(api_response_content$embedding$values)) {
      results[[i]] <- list(embedding = api_response_content$embedding$values)
    } else {
      # Store a placeholder that parse_embeddings can handle (e.g., leading to NAs)
      # The dimension of NA_real_ doesn't matter here, as parse_embeddings and get_batched_embeddings
      # will determine dimensionality from successful calls or handle it.
      results[[i]] <- list(embedding = NA_real_)
      if (verbose && !is.null(api_response_content)) {
        message("Unexpected response structure or missing embedding values for text ", i)
      } else if (verbose && is.null(api_response_content)) {
        message("No response content (likely due to API error) for text ", i)
      }
    }
  }

  # The 'json' parameter in the function signature is a bit tricky here.
  # we ignore it here
  final_output <- list(data = results)
  # If json=TRUE was intended to get the raw responses, this structure doesn't fully provide that
  # but this was really made for the generative calls!
  return(final_output)
}



# ----- Embedding Utility Functions -----

#' Parse Embedding Response into a Numeric Matrix
#'
#' Converts the embedding response data to a numeric matrix.
#'
#' @param embedding_response The response returned from an embedding API call.
#'
#' @return A numeric matrix of embeddings with column names as sequence numbers.
#' @export
#'
#' @examples
#' \dontrun{
#'   text_input <- c("Political science is a useful subject",
#'                   "We love sociology",
#'                   "German elections are different",
#'                   "A student was always curious.")
#'
#'   # Configure the embedding API provider (example with Voyage API)
#'   voyage_config <- llm_config(
#'     provider = "voyage",
#'     model = "voyage-large-2",
#'     api_key = Sys.getenv("VOYAGE_API_KEY")
#'   )
#'
#'   embedding_response <- call_llm(voyage_config, text_input)
#'   embeddings <- parse_embeddings(embedding_response)
#'   # Additional processing:
#'   embeddings |> cor() |> print()
#' }
parse_embeddings <- function(embedding_response) {
   if (is.null(embedding_response$data) || length(embedding_response$data) == 0)
     return(matrix(nrow = 0, ncol = 0))
   valid_embeddings_data <- purrr::keep(embedding_response$data, ~is.list(.x) && !is.null(.x$embedding) && !all(is.na(.x$embedding)))

  if (length(valid_embeddings_data) == 0)
    num_expected_rows <- length(embedding_response$data)


  list_of_vectors <- purrr::map(embedding_response$data, ~ {
    if (is.list(.x) && !is.null(.x$embedding) && !all(is.na(.x$embedding))) {
      as.numeric(.x$embedding)
    } else {
      NA_real_ # This will be treated as a vector of length 1 by list_transpose if not handled
    }
  })

  first_valid_vector <- purrr::detect(list_of_vectors, ~!all(is.na(.x)))
  true_embedding_dim <- if (!is.null(first_valid_vector)) length(first_valid_vector) else 0

  processed_list_of_vectors <- purrr::map(list_of_vectors, ~ {
    if (length(.x) == 1 && all(is.na(.x))) { # Was a placeholder for a failed embedding
      if (true_embedding_dim > 0) rep(NA_real_, true_embedding_dim) else NA_real_ # vector of NAs
    } else if (length(.x) == true_embedding_dim) {
      .x # Already correct
    } else {
      # This case should ideally not happen if API is consistent or errors are NA_real_
      if (true_embedding_dim > 0) rep(NA_real_, true_embedding_dim) else NA_real_
    }
  })

  if (true_embedding_dim == 0 && length(processed_list_of_vectors) > 0) {
    # All embeddings failed, and we couldn't determine dimension.
    # Return a matrix of NAs with rows = num_texts_in_batch, cols = 1 (placeholder)
    # get_batched_embeddings will later reconcile this with first_emb_dim if known from other batches.
    return(matrix(NA_real_, nrow = length(processed_list_of_vectors), ncol = 1))
  }
  if (length(processed_list_of_vectors) == 0) { # No data to process
    return(matrix(nrow = 0, ncol = 0))
  }

  embeddings_matrix <- processed_list_of_vectors |>
    purrr::list_transpose() |>
    as.data.frame() |>
    as.matrix()

  return(embeddings_matrix)
}








#' Generate Embeddings in Batches
#'
#' A wrapper function that processes a list of texts in batches to generate embeddings,
#' avoiding rate limits. This function calls \code{\link{call_llm_robust}} for each
#' batch and stitches the results together.
#'
#' @param texts Character vector of texts to embed. If named, the names will be
#'   used as row names in the output matrix.
#' @param embed_config An \code{llm_config} object configured for embeddings.
#' @param batch_size Integer. Number of texts to process in each batch. Default is 50.
#' @param verbose Logical. If TRUE, prints progress messages. Default is TRUE.
#'
#' @return A numeric matrix where each row is an embedding vector for the corresponding text.
#'   If embedding fails for certain texts, those rows will be filled with NA values.
#'   The matrix will always have the same number of rows as the input texts.
#'   Returns NULL if no embeddings were successfully generated.
#'
#' @export
#'
#' @examples
#' \dontrun{
#'   # Basic usage
#'   texts <- c("Hello world", "How are you?", "Machine learning is great")
#'   names(texts) <- c("greeting", "question", "statement")
#'
#'   embed_cfg <- llm_config(
#'     provider = "voyage",
#'     model = "voyage-large-2-instruct",
#'     embedding = TRUE,
#'     api_key = Sys.getenv("VOYAGE_API_KEY")
#'   )
#'
#'   embeddings <- get_batched_embeddings(
#'     texts = texts,
#'     embed_config = embed_cfg,
#'     batch_size = 2
#'   )
#' }
get_batched_embeddings <- function(texts,
                                   embed_config,
                                   batch_size = 50,
                                   verbose = FALSE) {

  # Input validation
  if (!is.character(texts) || length(texts) == 0) {
    if (verbose) message("No texts provided. Returning NULL.")
    return(NULL)
  }
  if (!inherits(embed_config, "llm_config")) {
    stop("embed_config must be a valid llm_config object.")
  }

  # Setup
  n_docs <- length(texts)
  batches <- split(seq_len(n_docs), ceiling(seq_len(n_docs) / batch_size))
  emb_list <- vector("list", n_docs)
  first_emb_dim <- NULL

  if (verbose) {
    message("Processing ", n_docs, " texts in ", length(batches), " batches of up to ", batch_size, " texts each")
  }

  # Process batches
  for (b in seq_along(batches)) {
    idx <- batches[[b]]
    batch_texts <- texts[idx]

    if (verbose) {
      message("Processing batch ", b, "/", length(batches), " (texts ", min(idx), "-", max(idx), ")")
    }

    tryCatch({
      # Call LLM for this batch using the robust caller
      resp <- call_llm_robust(embed_config, batch_texts, verbose = FALSE, json = TRUE)
      emb_chunk <- parse_embeddings(resp)

      if (is.null(first_emb_dim)) {
        first_emb_dim <- ncol(emb_chunk)
      }

      # Store per-document embeddings
      for (i in seq_along(idx)) {
        emb_list[[idx[i]]] <- emb_chunk[i, ]
      }

    }, error = function(e) {
      if (verbose) {
        message("Error in batch ", b, ": ", conditionMessage(e))
        message("Skipping batch and continuing...")
      }
      # Store NA for failed batch
      for (i in idx) {
        emb_list[[i]] <- NA
      }
    })
  }

  # Determine the dimension of the embeddings from the first successful result
  if (is.null(first_emb_dim)) {
    # Find the first non-NA element to determine dimensionality
    successful_emb <- purrr::detect(emb_list, ~ !all(is.na(.x)))
    if (!is.null(successful_emb)) {
      first_emb_dim <- length(successful_emb)
    } else {
      if (verbose) message("No embeddings were successfully generated.")
      return(NULL)
    }
  }

  # Replace NA placeholders with vectors of NAs of the correct dimension
  emb_list <- lapply(emb_list, function(emb) {
    if (length(emb) == 1 && is.na(emb)) {
      return(rep(NA_real_, first_emb_dim))
    }
    return(emb)
  })

  # Combine all embeddings into final matrix
  final_embeddings <- do.call(rbind, emb_list)

  if(!is.null(names(texts))){
    rownames(final_embeddings) <- names(texts)
  }

  if (verbose) {
    n_successful <- sum(stats::complete.cases(final_embeddings))
    message("Successfully generated embeddings for ", n_successful,
            "/", n_docs, " texts (", ncol(final_embeddings), " dimensions)")
  }

  return(final_embeddings)
}
############ Zagent.R
# -----------------------------------------------------------------------------
# ZAGENT.R
#
# This file contains the Agent and LLMConversation classes for building multi-agent
# conversational simulations. Agents can be added to a conversation, share memory,
# and respond to prompts
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# UTILITY
# -----------------------------------------------------------------------------

#' @keywords internal
#' Null-coalescing operator
`%||%` <- function(x, y) if (is.null(x)) y else x

# -----------------------------------------------------------------------------
# S3 "PLUS" DISPATCH FOR LLMConversation
# -----------------------------------------------------------------------------

#' @export
`+.LLMConversation` <- function(e1, e2) {
  # e1 = LLMConversation, e2 = something
  if (inherits(e2, "AgentAction")) {
    return(`+.LLMConversation.AgentAction`(e1, e2))
  } else {
    stop("Unsupported addition: LLMConversation + ", class(e2)[1])
  }
}

#' @export
`+.LLMConversation.AgentAction` <- function(e1, e2) {
  # e1 is LLMConversation, e2 is AgentAction
  agent_id <- e2$agent$id
  # If agent isn't in conversation, add it
  if (is.null(e1$agents[[agent_id]])) {
    e1$add_agent(e2$agent)
  }
  # Capture the agent's response
  response <- e1$converse(
    agent_id        = agent_id,
    prompt_template = e2$prompt_template,
    replacements    = e2$replacements,
    verbose         = e2$verbose
  )
  e1$last_response <- response

  # Update cumulative token counts
  e1$total_tokens_sent <- (e1$total_tokens_sent %||% 0) + response$tokens_sent
  e1$total_tokens_received <- (e1$total_tokens_received %||% 0) + response$tokens_received

  e1
}

# -----------------------------------------------------------------------------
# AGENTACTION S3 CLASS
# -----------------------------------------------------------------------------

#' AgentAction S3 Class
#'
#' @description
#' An object that bundles an Agent together with a prompt and replacements so that
#' it can be chained onto a conversation with the `+` operator.
#'
#' When `conversation + AgentAction` is called:
#' \enumerate{
#'   \item If the agent is not yet in the conversation, it is added.
#'   \item The agent is prompted with the provided prompt template (and replacements).
#'   \item The conversation is updated with the agent's response.
#' }
#'
#' @param agent An \code{Agent} object.
#' @param prompt_template A character string (the prompt).
#' @param replacements A named list for placeholder substitution (optional).
#' @param verbose Logical. If \code{TRUE}, prints verbose LLM response info. Default \code{FALSE}.
#'
#' @return An object of class \code{AgentAction}, used in conversation chaining.
#'
#' @export
AgentAction <- function(agent, prompt_template, replacements = list(), verbose = FALSE) {
  structure(
    list(
      agent           = agent,
      prompt_template = prompt_template,
      replacements    = replacements,
      verbose         = verbose
    ),
    class = "AgentAction"
  )
}

# -----------------------------------------------------------------------------
# AGENT R6 CLASS
# -----------------------------------------------------------------------------

#' @title Agent Class for LLM Interactions
#'
#' @description
#' An R6 class representing an agent that interacts with language models.
#'
#' *At agent-level we do not automate summarization.* The `maybe_summarize_memory()`
#'  function can be called manually if the user wishes to compress the agent's memory.
#'
#' @export
Agent <- R6::R6Class(
  "Agent",

  public = list(

    #' @field id Unique ID for this Agent.
    id = NULL,

    #' @field context_length Maximum number of conversation turns stored in memory.
    context_length = 5,

    #' @field model_config The \code{llm_config} specifying which LLM to call.
    model_config = NULL,

    #' @field memory A list of speaker/text pairs that the agent has memorized.
    memory = list(),

    #' @field persona Named list for additional agent-specific details (e.g., role, style).
    persona = list(),

    #' @field enable_summarization Logical. If TRUE, user *may* call `maybe_summarize_memory()`.
    enable_summarization = TRUE,

    #' @field token_threshold Numeric. If manually triggered, we can compare total_tokens.
    token_threshold = 1000,

    #' @field total_tokens Numeric. Estimated total tokens in memory.
    total_tokens = 0,

    #' @field summarization_density Character. "low", "medium", or "high".
    summarization_density = "medium",

    #' @field summarization_prompt Character. Optional custom prompt for summarization.
    summarization_prompt = NULL,

    #' @field summarizer_config Optional \code{llm_config} for summarizing the agent's memory.
    summarizer_config = NULL,

    #' @field auto_inject_conversation Logical. If TRUE, automatically prepend conversation memory if missing.
    auto_inject_conversation = TRUE,

    #' @description
    #' Create a new Agent instance.
    #'
    #' @param id Character. The agent's unique identifier.
    #' @param context_length Numeric. The maximum number of messages stored (default = 5).
    #' @param persona A named list of persona details.
    #' @param model_config An \code{llm_config} object specifying LLM settings.
    #' @param enable_summarization Logical. If TRUE, you can manually call summarization.
    #' @param token_threshold Numeric. If you're calling summarization, use this threshold if desired.
    #' @param summarization_density Character. "low", "medium", "high" for summary detail.
    #' @param summarization_prompt Character. Optional custom prompt for summarization.
    #' @param summarizer_config Optional \code{llm_config} for summarization calls.
    #' @param auto_inject_conversation Logical. If TRUE, auto-append conversation memory to prompt if missing.
    #'
    #' @return A new \code{Agent} object.
    initialize = function(id,
                          context_length = 5,
                          persona = NULL,
                          model_config,
                          enable_summarization = TRUE,
                          token_threshold = 1000,
                          summarization_density = "medium",
                          summarization_prompt = NULL,
                          summarizer_config = NULL,
                          auto_inject_conversation = TRUE) {

      if (missing(id)) stop("Agent id is required.")
      if (missing(model_config)) stop("model_config is required.")
      if (!inherits(model_config, "llm_config")) {
        stop("model_config must be an llm_config object.")
      }

      self$id <- id
      self$context_length <- context_length
      self$persona <- persona %||% list()
      self$model_config <- model_config
      self$memory <- list()

      self$enable_summarization <- enable_summarization
      self$token_threshold <- token_threshold
      self$total_tokens <- 0
      self$summarization_density <- match.arg(summarization_density, c("low", "medium", "high"))
      self$summarization_prompt <- summarization_prompt
      self$summarizer_config <- summarizer_config

      self$auto_inject_conversation <- auto_inject_conversation
    },

    #' @description
    #' Add a new message to the agent's memory.
    #' We do NOT automatically call summarization here.
    #'
    #' @param speaker Character. The speaker name or ID.
    #' @param text Character. The message content.
    add_memory = function(speaker, text) {
      # Estimate tokens: naive approach ~ words * 1.3
      new_tokens <- length(strsplit(text, "\\s+")[[1]]) * 1.3
      self$total_tokens <- self$total_tokens + new_tokens
      self$memory <- append(self$memory, list(list(speaker = speaker, text = text)))
    },

    #' @description
    #' Manually compress the agent's memory if desired.
    #' Summarizes all memory into a single "summary" message.
    maybe_summarize_memory = function() {
      if (!self$enable_summarization) {
        message("Summarization is disabled for this agent.")
        return(invisible())
      }
      if (length(self$memory) == 0) {
        message("No memory to summarize.")
        return(invisible())
      }

      conversation_text <- paste(
        sapply(self$memory, function(msg) paste0(msg$speaker, ": ", msg$text)),
        collapse = "\n"
      )

      density_instruction <- switch(
        self$summarization_density,
        "low"    = "Provide a concise summary with minimal detail.",
        "medium" = "Summarize the key points and main ideas.",
        "high"   = "Create a detailed summary including significant details."
      )

      # If user provided a summarization_prompt, use it. Otherwise default
      prompt <- self$summarization_prompt %||% paste(
        "Summarize this conversation:\n\n",
        conversation_text,
        "\n\n", density_instruction
      )

      # If we have a summarizer_config, use that. Otherwise, use the agent's own model_config
      if (!is.null(self$summarizer_config)) {
        summary_response <- call_llm_robust(
          self$summarizer_config,
          list(list(role = "user", content = prompt)),
          verbose = FALSE,
          json = TRUE
        )
        summary_text <- extract_text(attr(summary_response, "full_response"))
      } else {
        # Fallback: use the agent's own model_config
        tmp <- self$call_llm_agent(prompt, verbose = FALSE)
        summary_text <- tmp$text
      }

      # Reset and store only the summary
      self$reset_memory()
      self$add_memory("summary", summary_text)
      self$total_tokens <- length(strsplit(summary_text, "\\s+")[[1]]) * 1.3
      message("Agent memory has been summarized.")
    },

    #' @description
    #' Internal helper to prepare final prompt by substituting placeholders.
    #'
    #' @param template Character. The prompt template.
    #' @param replacements A named list of placeholder values.
    #' @return Character. The prompt with placeholders replaced.
    generate_prompt = function(template, replacements = list()) {
      replace_placeholders <- function(templ, reps) {
        if (length(reps) == 0) return(templ)
        out <- templ
        for (nm in names(reps)) {
          ph <- paste0("{{", nm, "}}")
          out <- gsub(ph, as.character(reps[[nm]]), out, fixed = TRUE)
        }
        out
      }
      replace_placeholders(template, replacements)
    },

    #' @description
    #' Low-level call to the LLM (via robust call_llm_robust) with a final prompt.
    #' If persona is defined, a system message is prepended to help set the role.
    #'
    #' @param prompt Character. The final prompt text.
    #' @param verbose Logical. If TRUE, prints debug info. Default FALSE.
    #'
    #' @return A list with:
    #'   * text
    #'   * tokens_sent
    #'   * tokens_received
    #'   * full_response (raw list)
    call_llm_agent = function(prompt, verbose = FALSE) {
      if (length(self$persona) > 0) {
        role <- self$persona$role %||% "agent"
        other_attrs <- self$persona[names(self$persona) != "role"]
        attrs_str <- if (length(other_attrs) > 0) {
          paste(names(other_attrs), "=", other_attrs, collapse = ", ")
        } else {
          ""
        }
        persona_str <- paste0(
          'Pretend you are a "', role, '"',
          if (nchar(attrs_str) > 0) paste0(" with: ", attrs_str, ".") else "."
        )
        messages <- list(
          list(role = "system", content = persona_str),
          list(role = "user", content = prompt)
        )
      } else {
        messages <- list(list(role = "user", content = prompt))
      }

      response <- tryCatch({
        call_llm_robust(self$model_config, messages, json = TRUE, verbose = verbose)
      }, error = function(e) {
        stop("LLM API call failed: ", e$message)
      })

      # Extract text
      full_resp <- attr(response, "full_response")
      text_out <- ""
      tryCatch({
        text_out <- private$extract_text_from_response(full_resp)
      }, error = function(e) {
        warning("Error extracting text: ", e$message)
      })

      # Token usage
      token_info <- list(tokens_sent = 0, tokens_received = 0)
      tryCatch({
        token_info <- private$extract_token_counts(full_resp, self$model_config$provider)
      }, error = function(e) {
        warning("Error counting tokens: ", e$message)
      })

      list(
        text            = if (is.null(text_out)) "" else as.character(text_out),
        tokens_sent     = as.numeric(token_info$tokens_sent),
        tokens_received = as.numeric(token_info$tokens_received),
        full_response   = full_resp
      )
    },

    #' @description
    #' Generate a response from the LLM using a prompt template and optional replacements.
    #' Substitutes placeholders, calls the LLM, saves output to memory, returns the response.
    #'
    #' @param prompt_template Character. The prompt template.
    #' @param replacements A named list of placeholder values.
    #' @param verbose Logical. If TRUE, prints extra info. Default FALSE.
    #'
    #' @return A list with fields \code{text}, \code{tokens_sent}, \code{tokens_received}, \code{full_response}.
    generate = function(prompt_template, replacements = list(), verbose = FALSE) {
      prompt <- self$generate_prompt(prompt_template, replacements)
      out <- self$call_llm_agent(prompt, verbose)
      # Append LLM's output to memory
      self$add_memory(self$id, out$text)
      out
    },

    #' @description
    #' The agent "thinks" about a topic, possibly using the entire memory in the prompt.
    #' If auto_inject_conversation is TRUE and the template lacks \{\{conversation\}\}, we prepend the memory.
    #'
    #' @param topic Character. Label for the thought.
    #' @param prompt_template Character. The prompt template.
    #' @param replacements Named list for additional placeholders.
    #' @param verbose Logical. If TRUE, prints info.
    think = function(topic, prompt_template, replacements = list(), verbose = FALSE) {
      if (missing(topic)) stop("Topic is required for thinking.")
      if (missing(prompt_template)) stop("Prompt template is required for thinking.")

      conversation <- paste(
        sapply(self$memory, function(msg) paste0(msg$speaker, ": ", msg$text)),
        collapse = "\n"
      )
      last_output <- if (length(self$memory) > 0) self$memory[[length(self$memory)]]$text else ""

      full_replacements <- c(
        list(topic = topic,
             conversation = conversation,
             last_output = last_output),
        replacements,
        self$persona
      )

      if (!grepl("\\{\\{conversation\\}\\}", prompt_template, fixed = TRUE) &&
          self$auto_inject_conversation) {
        if (length(self$memory) >0 )
          prompt_template <- paste(
            "Conversation so far:\n{{conversation}}\n\nNow think:\n",
            prompt_template)
      }

      self$generate(prompt_template, full_replacements, verbose)
    },

    #' @description
    #' The agent produces a public "response" about a topic.
    #' If auto_inject_conversation is TRUE and the template lacks \{\{conversation\}\}, we prepend the memory.
    #'
    #' @param topic Character. A short label for the question/issue.
    #' @param prompt_template Character. The prompt template.
    #' @param replacements Named list of placeholder substitutions.
    #' @param verbose Logical. If TRUE, prints extra info.
    #'
    #' @return A list with \code{text}, \code{tokens_sent}, \code{tokens_received}, \code{full_response}.
    respond = function(topic, prompt_template, replacements = list(), verbose = FALSE) {
      if (missing(topic)) stop("Topic is required for responding.")
      if (missing(prompt_template)) stop("Prompt template is required for responding.")

      conversation <- paste(
        sapply(self$memory, function(msg) paste0(msg$speaker, ": ", msg$text)),
        collapse = "\n"
      )
      last_output <- if (length(self$memory) > 0) self$memory[[length(self$memory)]]$text else ""

      full_replacements <- c(
        list(topic = topic,
             conversation = conversation,
             last_output = last_output),
        replacements,
        self$persona
      )

      if (!grepl("\\{\\{conversation\\}\\}", prompt_template, fixed = TRUE) &&
          self$auto_inject_conversation) {

        if ( length(self$memory)>0  )
          prompt_template <- paste(
            "Conversation so far:\n{{conversation}}\n\nNow respond:\n",
            prompt_template)
      }

      self$generate(prompt_template, full_replacements, verbose)
    },

    #' @description
    #' Reset the agent's memory.
    reset_memory = function() {
      self$memory <- list()
      self$total_tokens <- 0
    }
  ),

  private = list(

    extract_token_counts = function(response, provider) {
      usage <- response$usage %||% NULL
      if (is.null(usage)) {
        if (!is.null(response$usageMetadata)) {
          return(list(
            tokens_sent = as.numeric(response$usageMetadata$promptTokenCount %||% 0),
            tokens_received = as.numeric(response$usageMetadata$candidatesTokenCount %||% 0)
          ))
        }
        return(list(tokens_sent = 0, tokens_received = 0))
      }

      tokens_sent <- switch(
        provider,
        "openai"    = usage$prompt_tokens,
        "anthropic" = usage$input_tokens,
        "groq"      = usage$prompt_tokens,
        "together"  = usage$prompt_tokens,
        "deepseek"  = usage$prompt_tokens,
        "gemini"    = if (!is.null(response$usageMetadata)) {
          response$usageMetadata$promptTokenCount
        } else {
          usage$prompt_tokens
        },
        usage$prompt_tokens %||% 0
      )

      tokens_received <- switch(
        provider,
        "openai"    = usage$completion_tokens,
        "anthropic" = usage$output_tokens,
        "groq"      = usage$completion_tokens,
        "together"  = usage$completion_tokens,
        "deepseek"  = usage$completion_tokens,
        "gemini"    = if (!is.null(response$usageMetadata)) {
          response$usageMetadata$candidatesTokenCount
        } else {
          usage$completion_tokens
        },
        usage$completion_tokens %||% 0
      )

      list(
        tokens_sent     = as.numeric(tokens_sent %||% 0),
        tokens_received = as.numeric(tokens_received %||% 0)
      )
    },

    extract_text_from_response = function(response) {
      if (is.null(response)) return("")
      if (!is.null(response$choices) && length(response$choices) > 0) {
        choice <- response$choices[[1]]
        if (is.list(choice$message) && !is.null(choice$message$content)) {
          return(as.character(choice$message$content))
        }
        if (!is.null(choice$text)) {
          return(as.character(choice$text))
        }
      }
      if (!is.null(response$content) && length(response$content) > 0) {
        content_item <- response$content[[1]]
        if (!is.null(content_item$text)) {
          return(as.character(content_item$text))
        }
      }
      if (!is.null(response$candidates) && length(response$candidates) > 0) {
        candidate <- response$candidates[[1]]
        if (!is.null(candidate$content) &&
            !is.null(candidate$content$parts) &&
            length(candidate$content$parts) > 0) {
          part_text <- candidate$content$parts[[1]]$text
          if (!is.null(part_text)) {
            return(as.character(part_text))
          }
        }
      }
      ""
    }
  )
)

# -----------------------------------------------------------------------------
# LLMCONVERSATION R6 CLASS
# -----------------------------------------------------------------------------

#' @title LLMConversation Class for Coordinating Agents
#'
#' @description
#' An R6 class for managing a conversation among multiple \code{Agent} objects.
#' Includes optional conversation-level summarization if `summarizer_config` is provided:
#'
#' \enumerate{
#'   \item \strong{summarizer_config:} A list that can contain:
#'       \itemize{
#'         \item \code{llm_config}: The \code{llm_config} used for the summarizer call (default a basic OpenAI).
#'         \item \code{prompt}: A custom summarizer prompt (default provided).
#'         \item \code{threshold}: Word-count threshold (default 3000 words).
#'         \item \code{summary_length}: Target length in words for the summary (default 400).
#'       }
#'   \item Once the total conversation word count exceeds `threshold`, a summarization is triggered.
#'   \item The conversation is replaced with a single condensed message that keeps track of who said what.
#' }
#'
#' @export
LLMConversation <- R6::R6Class(
  "LLMConversation",
  public = list(
    #' @field agents A named list of \code{Agent} objects.
    agents = list(),
    #' @field conversation_history A list of speaker/text pairs for the entire conversation.
    conversation_history = list(),
    #' @field conversation_history_full A list of speaker/text pairs for the entire conversation that is never modified and never used directly.
    conversation_history_full = list(),
    #' @field topic A short string describing the conversation's theme.
    topic = NULL,
    #' @field prompts An optional list of prompt templates (may be ignored).
    prompts = NULL,
    #' @field shared_memory Global store that is also fed into each agent's memory.
    shared_memory = list(),
    #' @field last_response last response received
    last_response = NULL,
    #' @field total_tokens_sent total tokens sent in conversation
    total_tokens_sent = 0,
    #' @field total_tokens_received total tokens received in conversation
    total_tokens_received = 0,
    #' @field summarizer_config Config list controlling optional conversation-level summarization.
    summarizer_config = NULL,

    #' @description
    #' Create a new conversation.
    #' @param topic Character. The conversation topic.
    #' @param prompts Optional named list of prompt templates.
    #' @param summarizer_config Optional list controlling conversation-level summarization.
    initialize = function(topic,
                          prompts = NULL,
                          summarizer_config = NULL) {
      if (missing(topic)) stop("Conversation topic is required.")
      self$topic <- topic
      self$prompts <- prompts
      self$agents <- list()
      self$conversation_history <- list()
      self$conversation_history_full <- list()
      self$shared_memory <- list()
      self$last_response <- NULL
      self$total_tokens_sent <- 0
      self$total_tokens_received <- 0
      self$summarizer_config <- summarizer_config
    },

    #' @description
    #' Add an \code{Agent} to this conversation. The agent is stored by \code{agent$id}.
    #' @param agent An Agent object.
    add_agent = function(agent) {
      if (!inherits(agent, "Agent")) {
        stop("add_agent() requires an object of class Agent.")
      }
      self$agents[[agent$id]] <- agent
    },

    #' @description
    #' Add a message to the global conversation log. Also appended to shared memory.
    #' Then possibly trigger summarization if configured.
    #' @param speaker Character. Who is speaking?
    #' @param text Character. What they said.
    add_message = function(speaker, text) {

      new_msg <- list(speaker = speaker, text = text)

      self$conversation_history <- append(self$conversation_history, list(new_msg))

      self$conversation_history_full <- append(self$conversation_history_full, list(new_msg))  # Always accumulate

      self$shared_memory <- append(self$shared_memory, list(new_msg))

      # Attempt conversation-level summarization if summarizer_config is provided
      if (!is.null(self$summarizer_config)) {
        self$maybe_summarize_conversation()
      }
    },

    #' @description
    #' Have a specific agent produce a response. The entire global conversation plus
    #' shared memory is temporarily loaded into that agent. Then the new message is
    #' recorded in the conversation. The agent's memory is then reset except for its new line.
    #'
    #' @param agent_id Character. The ID of the agent to converse.
    #' @param prompt_template Character. The prompt template for the agent.
    #' @param replacements A named list of placeholders to fill in the prompt.
    #' @param verbose Logical. If TRUE, prints extra info.
    converse = function(agent_id, prompt_template, replacements = list(), verbose = FALSE) {
      if (is.null(self$agents[[agent_id]])) {
        stop("Agent ", agent_id, " is not in the conversation.")
      }
      agent <- self$agents[[agent_id]]

      # Temporarily feed entire conversation history to the agent
      for (msg in self$conversation_history) {
        agent$add_memory(msg$speaker, msg$text)
      }
      # Also feed shared memory
      for (msg in self$shared_memory) {
        agent$add_memory("shared", msg$text)
      }

      # Let the agent respond
      response <- agent$respond(self$topic, prompt_template, replacements, verbose)

      # Store in the conversation
      self$add_message(agent_id, response$text)

      # Agent memory: reset then store only the new line
      agent$reset_memory()
      agent$add_memory(agent_id, response$text)

      invisible(response)
    },

    #' @description
    #' Run a multi-step conversation among a sequence of agents.
    #' @param agent_sequence Character vector of agent IDs in the order they speak.
    #' @param prompt_template Single string or named list of strings keyed by agent ID.
    #' @param replacements Single list or list-of-lists with per-agent placeholders.
    #' @param verbose Logical. If TRUE, prints extra info.
    run = function(agent_sequence, prompt_template, replacements = list(), verbose = FALSE) {
      if (is.null(agent_sequence)) {
        stop("agent_sequence cannot be NULL.")
      }
      for (i in seq_along(agent_sequence)) {
        agent_id <- agent_sequence[i]
        current_prompt <- if (is.list(prompt_template)) {
          prompt_template[[agent_id]] %||% stop("No prompt for agent: ", agent_id)
        } else {
          prompt_template
        }
        current_reps <- if (is.list(replacements) && length(replacements) == length(agent_sequence)) {
          replacements[[i]]
        } else {
          replacements
        }
        self$converse(agent_id, current_prompt, current_reps, verbose)
      }
    },

    #' @description
    #' Print the conversation so far to the console.
    print_history = function() {
      cat("Conversation History:\n")
      for (msg in self$conversation_history) {
        cat(sprintf("\n-------------\n<%s>: %s\n", msg$speaker, msg$text))
      }
    },

    #' @description
    #' Clear the global conversation and reset all agents' memories.
    reset_conversation = function() {
      self$conversation_history <- list()
      self$shared_memory <- list()
      for (agent in self$agents) {
        agent$reset_memory()
      }
      # Note: conversation_history_full remains untouched
    },

    #' @description
    #' Pipe-like operator to chain conversation steps. E.g., conv |> "Solver"(...)
    #'
    #' @param agent_id Character. The ID of the agent to call next.
    #'
    #' @return A function that expects (prompt_template, replacements, verbose).
    `|>` = function(agent_id) {
      if (!is.character(agent_id) || length(agent_id) != 1) {
        stop("agent_id must be a single character string.")
      }
      if (is.null(self$agents[[agent_id]])) {
        stop("Agent ", agent_id, " not in conversation.")
      }
      force(agent_id)
      function(prompt_template, replacements = list(), verbose = FALSE) {
        self$converse(agent_id, prompt_template, replacements, verbose)
      }
    },

    #' @description
    #' Possibly summarize the conversation if summarizer_config is non-null and
    #' the word count of conversation_history exceeds summarizer_config$threshold.
    maybe_summarize_conversation = function() {
      cfg <- self$summarizer_config

      # Defaults
      threshold <- cfg$threshold %||% 3000
      summary_length <- cfg$summary_length %||% 400

      total_words <- sum(
        sapply(self$conversation_history, function(x) {
          length(strsplit(x$text, "\\s+")[[1]])
        })
      )

      if (total_words > threshold) {
        self$summarize_conversation()
      }
    },

    #' @description
    #' Summarize the conversation so far into one condensed message.
    #' The new conversation history becomes a single message with speaker = "summary".
    summarize_conversation = function() {
      if (is.null(self$summarizer_config)) {
        return(invisible())
      }
      cfg <- self$summarizer_config
      threshold <- cfg$threshold %||% 3000
      summary_length <- cfg$summary_length %||% 400

      # Build a string: "Speaker: text" lines
      conversation_text <- paste(
        sapply(self$conversation_history, function(msg) {
          paste0(msg$speaker, ": ", msg$text)
        }),
        collapse = "\n"
      )

      # Default summarizer prompt if none given

      default_prompt <- paste0(
        "Summarize the following conversation into roughly ", summary_length, " words.\n",
        "Your summary should:\n",
        "- Identify each speaker by role or name.\n",
        "- Preserve their main arguments, concerns, and proposals, pieces of evidence, anecdotes, etc. \n",
        "- Keep important details that is relevant \n",
        "- Maintain a concise structure without omitting critical points.\n\n",
        "CONVERSATION:\n",
        conversation_text,
        "\n\nSUMMARY:"
      )

      prompt <- cfg$prompt %||% default_prompt

      # If summarizer_config has llm_config, use it; else raise an error
      if (!is.null(cfg$llm_config)) {
        summarizer_llm <- cfg$llm_config
      } else {
        stop("summarizer_config must contain an llm_config object.")
      }

      # Summarize
      summary_response <- call_llm_robust(
        summarizer_llm,
        list(list(role = "user", content = prompt)),
        json = TRUE
      )
      # Extract text
      full_resp <- attr(summary_response, "full_response")
      summary_text <- ""
      tryCatch({
        summary_text <- extract_text(full_resp)
      }, error = function(e) {
        warning("Error extracting summary text: ", e$message)
      })

      # Replace conversation with a single summary message
      self$conversation_history <- list(list(speaker = "summary", text = summary_text))
      self$shared_memory <- list(list(speaker = "summary", text = summary_text))
    }
  )
)
