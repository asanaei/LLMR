% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_chat_session.R
\name{chat_session}
\alias{chat_session}
\alias{as.data.frame.llm_chat_session}
\alias{summary.llm_chat_session}
\alias{head.llm_chat_session}
\alias{tail.llm_chat_session}
\title{Stateful chat session}
\usage{
chat_session(config, system = NULL, ...)

\method{as.data.frame}{llm_chat_session}(x, ...)

\method{summary}{llm_chat_session}(object, ...)

\method{head}{llm_chat_session}(x, n = 6L, width = getOption("width") - 15, ...)

\method{tail}{llm_chat_session}(x, n = 6L, width = getOption("width") - 15, ...)
}
\arguments{
\item{config}{An [\code{llm_config}] **for a generative model**
(i.e. \code{embedding = FALSE}).}

\item{system}{Optional system prompt inserted once at the beginning.}

\item{...}{Default arguments forwarded to every
[\code{call_llm_robust()}] call (e.g.
\code{verbose = TRUE}, \code{json = TRUE}).}
}
\value{
An object of class **\code{llm_chat_session}** with the methods
  listed below.
}
\description{
Create a lightweight, in-memory conversation object that retains message
history between calls to the LLM.  Internally it wraps
\code{call_llm_robust()} so you still benefit from retry logic,
caching, and error logging.
}
\section{Functions}{
\itemize{
\item \code{as.data.frame(llm_chat_session)}: Coerce a session to a two-column data frame.

\item \code{summary(llm_chat_session)}: Summary statistics for a chat session.

\item \code{head(llm_chat_session)}: First *n* rows of the conversation.

\item \code{tail(llm_chat_session)}: Last *n* rows of the conversation.

}}
\section{How it works}{

  1.  A private environment stores the running list of
      \code{list(role, content)} messages.
  2.  At each \code{$send()} the history is sent *in full* to the model.
  3.  Provider-agnostic token counts are extracted from the JSON response
      (fields are detected by name, so new providers continue to work).
}

\section{Public methods}{

\describe{
  \item{\code{$send(text, ..., role = "user")}}{
    Append a message (default role \code{"user"}), query the model,
    print the assistantâ€™s reply, and invisibly return it.}
  \item{\code{$history()}}{Raw list of messages.}
  \item{\code{$history_df()}}{Two-column data frame (\code{role},
    \code{content}).}
  \item{\code{$tokens_sent()}/\code{$tokens_received()}}{Running token
    totals.}
  \item{\code{$reset()}}{Clear history (retains the optional system
    message).}
}
}

\examples{
\dontrun{
cfg  <- llm_config("openai", "gpt-4o-mini", Sys.getenv("OPENAI_API_KEY"))
chat <- chat_session(cfg, system = "Be concise.")
chat$send("Who invented the moon?")
chat$send("Explain why in one short sentence.")
chat           # snapshot (first 10 turns)
tail(chat, 2)  # last 2 turns
}
}
