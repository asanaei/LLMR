[{"path":"https://asanaei.github.io/LLMR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 LLMR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"tldr","dir":"Articles","previous_headings":"","what":"TL;DR","title":"Schema-validated output in LLMR","text":"JSON mode: ask model “JSON object.” Lower friction. Weak guarantees. Schema output: give JSON Schema request strict validation. Higher reliability provider enforces . Reality: enforcement request shapes differ across providers. Use defensive parsing local validation.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"what-the-major-providers-actually-support","dir":"Articles","previous_headings":"","what":"What the major providers actually support","title":"Schema-validated output in LLMR","text":"OpenAI-compatible (OpenAI, Groq, Together, x.ai, DeepSeek) Chat Completions accept response_format (e.g., {\"type\":\"json_object\"} JSON-Schema payload). Enforcement varies provider interface OpenAI-shaped. See OpenAI API overview, Groq API (OpenAI-compatible), Together: OpenAI compatibility, x.ai: OpenAI API schema, DeepSeek: OpenAI-compatible endpoint Anthropic (Claude) global “JSON mode.” Instead, define tool input_schema (JSON Schema) force via tool_choice, model must return JSON object validates schema. See Anthropic Messages API: tools & input_schema Google Gemini (REST) Set responseMimeType = \"application/json\" generationConfig request JSON. models also accept responseSchema constrained JSON (model-dependent). See Gemini documentation —","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"why-prefer-schema-output","dir":"Articles","previous_headings":"","what":"Why prefer schema output?","title":"Schema-validated output in LLMR","text":"Deterministic downstream code: predictable keys/types enable typed transforms. Safer integrations: strict mode avoids extra keys, missing fields, textual preambles. Faster failure: invalid generations fail early, retry/backoff easy manage.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"why-json-only-still-matters","dir":"Articles","previous_headings":"","what":"Why JSON-only still matters","title":"Schema-validated output in LLMR","text":"Broadest support across models/providers/proxies. Low ceremony exploration, labeling, quick prototypes.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"quirks-you-will-hit-in-practice","dir":"Articles","previous_headings":"","what":"Quirks you will hit in practice","title":"Schema-validated output in LLMR","text":"Models often wrap JSON code fences add pre/post text. Arrays/objects appear expected scalars; ints vs doubles vary provider/sample. Safety/length caps can truncate output; detect handle “finish_reason = length/filter.”","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"llmr-helpers-to-blunt-those-edges","dir":"Articles","previous_headings":"Quirks you will hit in practice","what":"LLMR helpers to blunt those edges","title":"Schema-validated output in LLMR","text":"llm_parse_structured() strips fences extracts largest balanced {...} [...] parsing. llm_parse_structured_col() hoists fields (supports dot/bracket paths JSON Pointer) keeps non-scalars list-columns. llm_validate_structured_col() validates locally via jsonvalidate (AJV). enable_structured_output() flips right provider switch (OpenAI-compat response_format, Anthropic tool + input_schema, Gemini responseMimeType/responseSchema).","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"minimal-patterns-guarded-code","dir":"Articles","previous_headings":"","what":"Minimal patterns (guarded code)","title":"Schema-validated output in LLMR","text":"chunks use tiny helper document knits even without API keys.","code":"safe <- function(expr) tryCatch(expr, error = function(e) {message(\"ERROR: \", e$message); NULL})"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"json-mode-no-schema-works-across-openai-compatible-providers","dir":"Articles","previous_headings":"Minimal patterns (guarded code)","what":"1) JSON mode, no schema (works across OpenAI-compatible providers)","title":"Schema-validated output in LLMR","text":"still fail? Proxies labeled “OpenAI-compatible” sometimes accept response_format don’t strictly enforce ; LLMR’s parser recovers fences pre/post text.","code":"safe({   library(LLMR)   cfg <- llm_config(     provider = \"openai\",                # try \"groq\" or \"together\" too     model    = \"gpt-4.1-nano\",     temperature = 0   )    # Flip JSON mode on (OpenAI-compat shape)   cfg_json <- enable_structured_output(cfg, schema = NULL)    res    <- call_llm(cfg_json, 'Give me a JSON object {\"ok\": true, \"n\": 3}.')   parsed <- llm_parse_structured(res)    cat(\"Raw text:\\n\", as.character(res), \"\\n\\n\")   str(parsed) })"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"schema-mode-that-actually-works-groq-qwen-open-weights-non-commercial-friendly","dir":"Articles","previous_headings":"Minimal patterns (guarded code)","what":"2) Schema mode that actually works (Groq + Qwen, open-weights / non-commercial friendly)","title":"Schema-validated output in LLMR","text":"Groq serves Qwen 2.5 Instruct models OpenAI-compatible APIs. Structured Outputs feature enforces JSON Schema (notably) expects properties listed required. key set, see structured_ok = TRUE, structured_valid = TRUE, plus parsed columns. Common gotcha: Groq returns 400 error complaining required, ensure properties listed required array. Groq’s structured output implementation stricter OpenAI’s.","code":"safe({   library(LLMR); library(dplyr)    # Schema: make every property required to satisfy Groq's stricter check   schema <- list(     type = \"object\",     additionalProperties = FALSE,     properties = list(       title = list(type = \"string\"),       year  = list(type = \"integer\"),       tags  = list(type = \"array\", items = list(type = \"string\"))     ),     required = list(\"title\",\"year\",\"tags\")   )    cfg <- llm_config(     provider = \"groq\",     model    = \"qwen-2.5-72b-instruct\",   # a Qwen Instruct model on Groq     temperature = 0   )   cfg_strict <- enable_structured_output(cfg, schema = schema, strict = TRUE)    df  <- tibble(x = c(\"BERT paper\", \"Vision Transformers\"))   out <- llm_fn_structured(     df,     prompt   = \"Return JSON about '{x}' with fields title, year, tags.\",     .config  = cfg_strict,     .schema  = schema,          # send schema to provider     .fields  = c(\"title\",\"year\",\"tags\"),     .validate_local = TRUE   )    out %>% select(structured_ok, structured_valid, title, year, tags) %>% print(n = Inf) })"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"anthropic-force-a-schema-via-a-tool-may-require-max_tokens","dir":"Articles","previous_headings":"Minimal patterns (guarded code)","what":"3) Anthropic: force a schema via a tool (may require max_tokens)","title":"Schema-validated output in LLMR","text":"Anthropic requires max_tokens; LLMR warns defaults omit .","code":"safe({   library(LLMR)   schema <- list(     type=\"object\",     properties=list(answer=list(type=\"string\"), confidence=list(type=\"number\")),     required=list(\"answer\",\"confidence\"),     additionalProperties=FALSE   )    cfg <- llm_config(\"anthropic\",\"claude-3-5-haiku-latest\", temperature = 0)   cfg <- enable_structured_output(cfg, schema = schema, name = \"llmr_schema\")    res <- call_llm(cfg, c(     system = \"Return only the tool result that matches the schema.\",     user   = \"Answer: capital of Japan; include confidence in [0,1].\"   ))    parsed <- llm_parse_structured(res)   str(parsed) })"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"gemini-json-response-plus-optional-response-schema-on-supported-models","dir":"Articles","previous_headings":"Minimal patterns (guarded code)","what":"4) Gemini: JSON response (plus optional response schema on supported models)","title":"Schema-validated output in LLMR","text":"","code":"safe({   library(LLMR)    cfg <- llm_config(     \"gemini\", \"gemini-2.5-flash-lite\",     response_mime_type = \"application/json\"  # ask for JSON back     # Optionally: gemini_enable_response_schema = TRUE, response_schema = <your JSON Schema>   )    res <- call_llm(cfg, c(     system = \"Reply as JSON only.\",     user   = \"Produce fields name and score about 'MNIST'.\"   ))   str(llm_parse_structured(res)) })"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"defensive-patterns-no-api-calls","dir":"Articles","previous_headings":"","what":"Defensive patterns (no API calls)","title":"Schema-validated output in LLMR","text":"helps Works outputs arrive fenced, pre/post text, arrays sneak . Non-scalars become list-columns (set allow_list = FALSE force scalars ).","code":"safe({   library(LLMR); library(tibble)    messy <- c(     '```json\\n{\"x\": 1, \"y\": [1,2,3]}\\n```',     'Sure! Here is JSON: {\"x\":\"1\",\"y\":\"oops\"} trailing words',     '{\"x\":1, \"y\":[2,3,4]}'   )    tibble(response_text = messy) |>     llm_parse_structured_col(       fields = c(x = \"x\", y = \"/y/0\")   # dot/bracket or JSON Pointer     ) |>     print(n = Inf) })"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"pro-tip-combine-with-parallel-execution","dir":"Articles","previous_headings":"","what":"Pro tip: Combine with parallel execution","title":"Schema-validated output in LLMR","text":"production ETL workflows, combine schema validation parallelization: processes thousands rows efficiently automatic retries validation.","code":"library(LLMR); library(dplyr)  cfg_with_schema = llm_config('openai','gpt-4.1-nano')    setup_llm_parallel(workers = 10)  ### Assuming there is a large data frame large_df  large_df |>   llm_mutate_structured(     result,     prompt = \"Extract: {text}\",     .config = cfg_with_schema,     .schema = schema,     .fields = c(\"label\", \"score\"),     tries = 3  # auto-retry failures   )  reset_llm_parallel()"},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"choosing-the-mode","dir":"Articles","previous_headings":"","what":"Choosing the mode","title":"Schema-validated output in LLMR","text":"Reporting / ETL / metrics: Schema mode; fail fast retry. Exploration / ad-hoc: JSON mode + recovery parser. Cross-provider code: Always wrap provider toggles enable_structured_output() run llm_parse_structured() + local validation.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/about-schema.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Schema-validated output in LLMR","text":"OpenAI: Structure Output: https://platform.openai.com/docs/guides/structured-outputs Groq: Structured Outputs: https://console.groq.com/docs/structured-outputs Together: Structured Output: https://docs.together.ai/docs/json-mode x.ai: Structured Output: https://docs.x.ai/docs/guides/structured-outputs DeepSeek: JSON Mode: https://api-docs.deepseek.com/guides/json_mode Anthropic: Messages API, tools & input_schema: https://docs.claude.com/en/api/messages#body-tool-choice Google Gemini: Structured Output: https://ai.google.dev/gemini-api/docs/structured-output","code":""},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"openai-gpt-5-nano","dir":"Articles","previous_headings":"","what":"OpenAI: gpt-5-nano","title":"Simple chat with LLMR","text":"","code":"library(LLMR)  cfg_openai <- llm_config(   provider = \"openai\",   model    = \"gpt-5-nano\",    )  chat_oai <- chat_session(cfg_openai, system = \"Be concise.\") chat_oai$send(\"Say a warm hello in one short sentence.\") chat_oai$send(\"Now say it in Esperanto.\")"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"anthropic-claude-sonnet-4-20250514","dir":"Articles","previous_headings":"","what":"Anthropic: claude-sonnet-4-20250514","title":"Simple chat with LLMR","text":"","code":"cfg_anthropic <- llm_config(   provider = \"anthropic\",   model    = \"claude-sonnet-4-20250514\",   max_tokens = 512   # avoid warnings; Anthropic requires max_tokens )  chat_claude <- chat_session(cfg_anthropic, system = \"Be concise.\") chat_claude$send(\"Name one interesting fact about honey bees.\")"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"gemini-gemini-2-5-flash","dir":"Articles","previous_headings":"","what":"Gemini: gemini-2.5-flash","title":"Simple chat with LLMR","text":"","code":"cfg_gemini <- llm_config(   provider = \"gemini\",   model    = \"gemini-2.5-flash-lite\",    )  chat_gem <- chat_session(cfg_gemini, system = \"Be concise.\") chat_gem$send(\"Give me a single-sentence fun fact about volcanoes.\")"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"groq-openaigpt-oss-20b","dir":"Articles","previous_headings":"","what":"Groq: openai/gpt-oss-20b","title":"Simple chat with LLMR","text":"","code":"cfg_groq <- llm_config(   provider = \"groq\",   model    = \"openai/gpt-oss-20b\",    )  chat_groq <- chat_session(cfg_groq, system = \"Be concise.\") chat_groq$send(\"Share a short fun fact about octopuses.\")"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"using-the-chat-history","dir":"Articles","previous_headings":"","what":"Using the chat history","title":"Simple chat with LLMR","text":"Chat sessions remember context automatically:","code":"chat_oai$send(\"What did I ask you to do in my first message?\") # The model can reference the earlier \"Say a warm hello\" request"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"inspect-the-full-conversation","dir":"Articles","previous_headings":"","what":"Inspect the full conversation","title":"Simple chat with LLMR","text":"","code":"# View all messages as.data.frame(chat_oai)  # Get summary statistics summary(chat_oai)"},{"path":"https://asanaei.github.io/LLMR/articles/chat-basics.html","id":"structured-chat-in-one-call-openai-example","dir":"Articles","previous_headings":"","what":"Structured chat in one call (OpenAI example)","title":"Simple chat with LLMR","text":"","code":"schema <- list(   type = \"object\",   properties = list(     answer     = list(type = \"string\"),     confidence = list(type = \"number\")   ),   required = list(\"answer\", \"confidence\"),   additionalProperties = FALSE )  chat_oai$send_structured(   \"Return an answer and a confidence score (0-1) about: Why is the sky blue?\",   schema )"},{"path":"https://asanaei.github.io/LLMR/articles/experiments.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Small experiment with LLMR","text":"vignette demonstrates: Building factorial experiment designs build_factorial_experiments() Running experiments parallel call_llm_par() Comparing unstructured vs. structured output across providers workflow : design → parallel execution → analysis compare three configurations two prompts, unstructured structured output. choosing models, note time writing vignette, Gemini models guaranteeing schema output likely run trouble.","code":"library(LLMR) library(dplyr) cfg_openai <- llm_config(\"openai\",   \"gpt-5-nano\") cfg_cld    <- llm_config(\"anthropic\",\"claude-sonnet-4-20250514\", max_tokens = 512) cfg_gem    <- llm_config(\"groq\",     \"openai/gpt-oss-20b\")"},{"path":"https://asanaei.github.io/LLMR/articles/experiments.html","id":"build-a-factorial-design","dir":"Articles","previous_headings":"","what":"Build a factorial design","title":"Small experiment with LLMR","text":"","code":"experiments <- build_factorial_experiments(   configs       = list(cfg_openai, cfg_cld, cfg_gem),   user_prompts  = c(\"Summarize in one sentence: The Apollo program.\",                     \"List two benefits of green tea.\"),   system_prompts = c(\"Be concise.\") ) experiments"},{"path":"https://asanaei.github.io/LLMR/articles/experiments.html","id":"run-unstructured","dir":"Articles","previous_headings":"","what":"Run unstructured","title":"Small experiment with LLMR","text":"Understanding results: finish_reason column shows response ended: \"stop\": normal completion \"length\": hit token limit (increase max_tokens) \"filter\": content filter triggered user_prompt_label helps track experimental condition produced response.","code":"setup_llm_parallel(workers = 10) res_unstructured <- call_llm_par(experiments, progress = TRUE) reset_llm_parallel() res_unstructured |>   select(provider, model, user_prompt_label, response_text, finish_reason) |>   head()"},{"path":"https://asanaei.github.io/LLMR/articles/experiments.html","id":"structured-version","dir":"Articles","previous_headings":"","what":"Structured version","title":"Small experiment with LLMR","text":"","code":"schema <- list(   type = \"object\",   properties = list(     answer = list(type=\"string\"),     keywords = list(type=\"array\", items = list(type=\"string\"))   ),   required = list(\"answer\",\"keywords\"),   additionalProperties = FALSE )  experiments2 <- experiments experiments2$config <- lapply(experiments2$config, enable_structured_output, schema = schema)  setup_llm_parallel(workers = 10) res_structured <- call_llm_par_structured(experiments2 , .fields = c(\"answer\",\"keywords\") ) reset_llm_parallel()  res_structured |>   select(provider, model, user_prompt_label, structured_ok, answer) |>   head()"},{"path":"https://asanaei.github.io/LLMR/articles/presidential_embeddings.html","id":"prepare-the-text-data","dir":"Articles","previous_headings":"","what":"Prepare the Text Data","title":"Presidential Speech Analysis with Embeddings","text":"’ll analyze excerpts several U.S. presidential inaugural addresses:","code":"text_input <- c(   Washington = \"Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years--a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies. In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected. All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow-citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences be judged by my country with some share of the partiality in which they originated.\",   Adams = \"When it was first perceived, in early times, that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims, men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country. Relying, however, on the purity of their intentions, the justice of their cause, and the integrity and intelligence of the people, under an overruling Providence which had so signally protected this country from the first, the representatives of this nation, then consisting of little more than half its present number, not only broke to pieces the chains which were forging and the rod of iron that was lifted up, but frankly cut asunder the ties which had bound them, and launched into an ocean of uncertainty.\",   Jefferson = \"Called upon to undertake the duties of the first executive office of our country, I avail myself of the presence of that portion of my fellow-citizens which is here assembled to express my grateful thanks for the favor with which they have been pleased to look toward me, to declare a sincere consciousness that the task is above my talents, and that I approach it with those anxious and awful presentiments which the greatness of the charge and the weakness of my powers so justly inspire. A rising nation, spread over a wide and fruitful land, traversing all the seas with the rich productions of their industry, engaged in commerce with nations who feel power and forget right, advancing rapidly to destinies beyond the reach of mortal eye -- when I contemplate these transcendent objects, and see the honor, the happiness, and the hopes of this beloved country committed to the issue and the auspices of this day, I shrink from the contemplation, and humble myself before the magnitude of the undertaking. Utterly, indeed, should I despair did not the presence of many whom I here see remind me that in the other high authorities provided by our Constitution I shall find resources of wisdom, of virtue, and of zeal on which to rely under all difficulties. To you, then, gentlemen, who are charged with the sovereign functions of legislation, and to those associated with you, I look with encouragement for that guidance and support which may enable us to steer with safety the vessel in which we are all embarked amidst the conflicting elements of a troubled world.\",   Madison = \"Unwilling to depart from examples of the most revered authority, I avail myself of the occasion now presented to express the profound impression made on me by the call of my country to the station to the duties of which I am about to pledge myself by the most solemn of sanctions. So distinguished a mark of confidence, proceeding from the deliberate and tranquil suffrage of a free and virtuous nation, would under any circumstances have commanded my gratitude and devotion, as well as filled me with an awful sense of the trust to be assumed. Under the various circumstances which give peculiar solemnity to the existing period, I feel that both the honor and the responsibility allotted to me are inexpressibly enhanced.\",   Bush = \"The peaceful transfer of authority is rare in history, yet common in our country. With a simple oath, we affirm old traditions and make new beginnings. As I begin, I thank President Clinton for his service to our Nation, and I thank Vice President Gore for a contest conducted with spirit and ended with grace. I am honored and humbled to stand here where so many of America's leaders have come before me, and so many will follow. We have a place, all of us, in a long story, a story we continue but whose end we will not see. It is a story of a new world that became a friend and liberator of the old, the story of a slaveholding society that became a servant of freedom, the story of a power that went into the world to protect but not possess, to defend but not to conquer.\",   Obama = \"My fellow citizens, I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our Nation, as well as the generosity and cooperation he has shown throughout this transition. Forty-four Americans have now taken the Presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet every so often, the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because we the people have remained faithful to the ideals of our forebears and true to our founding documents.\",   Trump = \"We, the citizens of America, are now joined in a great national effort to rebuild our country and restore its promise for all of our people. Together, we will determine the course of America and the world for many, many years to come. We will face challenges, we will confront hardships, but we will get the job done. Every 4 years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition. They have been magnificent. Thank you.\",   Biden = \"This is America's day. This is democracy's day, a day of history and hope, of renewal and resolve. Through a crucible for the ages America has been tested anew, and America has risen to the challenge. Today we celebrate the triumph not of a candidate, but of a cause, the cause of democracy. The people—the will of the people has been heard, and the will of the people has been heeded. We've learned again that democracy is precious, democracy is fragile. And at this hour, my friends, democracy has prevailed.\" )"},{"path":"https://asanaei.github.io/LLMR/articles/presidential_embeddings.html","id":"configure-embedding-model","dir":"Articles","previous_headings":"","what":"Configure Embedding Model","title":"Presidential Speech Analysis with Embeddings","text":"","code":"embed_cfg <- llm_config(   provider  = \"gemini\",   model     = \"embedding-001\", # or \"text-embedding-004\",   task_type = \"CLASSIFICATION\",   embedding = TRUE )"},{"path":"https://asanaei.github.io/LLMR/articles/presidential_embeddings.html","id":"generate-embeddings","dir":"Articles","previous_headings":"","what":"Generate Embeddings","title":"Presidential Speech Analysis with Embeddings","text":"","code":"embeddings <- get_batched_embeddings(   texts = text_input,   embed_config = embed_cfg,   batch_size = 5  )"},{"path":"https://asanaei.github.io/LLMR/articles/presidential_embeddings.html","id":"analyze-similarity","dir":"Articles","previous_headings":"","what":"Analyze Similarity","title":"Presidential Speech Analysis with Embeddings","text":"","code":"# Compute correlation matrix cors <- cor(t(embeddings)) print(round(cors, 2)) # Normalize embeddings for cosine similarity embd_normalized <- t(apply(embeddings, 1,                            function(x) x / sqrt(sum(x^2))))  # Compute cosine similarity matrix sim_matrix <- embd_normalized %*% t(embd_normalized)  # Convert similarity to distance dist_matrix <- 1 - sim_matrix  # Convert to a distance object dist_object <- as.dist(dist_matrix)  # Perform hierarchical clustering hc <- hclust(dist_object, method = \"ward.D2\") plot(hc, main = \"Clustering of Presidential Inaugural Speeches\\n(only beginning the paragraphs)\")"},{"path":"https://asanaei.github.io/LLMR/articles/presidential_embeddings.html","id":"expected-results","dir":"Articles","previous_headings":"","what":"Expected Results","title":"Presidential Speech Analysis with Embeddings","text":"run valid API key, analysis : Generate high-dimensional embeddings presidential speech excerpt Show correlation patterns different presidents’ rhetorical styles Create dendrogram clustering presidents linguistic similarity Reveal historical stylistic patterns presidential rhetoric run vignette : Set Gemini API key Sys.setenv(GEMINI_API_KEY = \"your_key_here\") running code chunks.","code":""},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_fn-unstructured-openai","dir":"Articles","previous_headings":"","what":"llm_fn: unstructured (OpenAI)","title":"Tidy pipelines and structured output","text":"","code":"words <- c(\"excellent\", \"awful\", \"fine\") out <- llm_fn(   words,   prompt  = \"Classify '{x}' as Positive, Negative, or Neutral.\",   .config = cfg_openai,   .return = \"columns\" ) out"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_fn-unstructured-groq","dir":"Articles","previous_headings":"","what":"llm_fn: unstructured (Groq)","title":"Tidy pipelines and structured output","text":"","code":"out_groq <- llm_fn(   words,   prompt  = \"Classify '{x}' as Positive, Negative, or Neutral.\",   .config = cfg_groq,   .return = \"columns\" ) out_groq"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_fn_structured-schema-first-openai","dir":"Articles","previous_headings":"","what":"llm_fn_structured: schema-first (OpenAI)","title":"Tidy pipelines and structured output","text":"","code":"schema <- list(   type = \"object\",   properties = list(     label = list(type = \"string\", description = \"Sentiment label\"),     score = list(type = \"number\", description = \"Confidence 0..1\")   ),   required = list(\"label\", \"score\"),   additionalProperties = FALSE )  out_s <- llm_fn_structured(   x = words,   prompt  = \"Classify '{x}' as Positive, Negative, or Neutral with confidence.\",   .config = cfg_openai,   .schema = schema,   .fields = c(\"label\", \"score\") ) out_s"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_mutate-unstructured-anthropic","dir":"Articles","previous_headings":"","what":"llm_mutate: unstructured (Anthropic)","title":"Tidy pipelines and structured output","text":"","code":"df <- tibble::tibble(   id   = 1:3,   text = c(\"Cats are great pets\", \"The weather is bad\", \"I like tea\") )  df_u <- df |>   llm_mutate(     answer  = \"Give a short category for: {text}\",     .config = cfg_cld,     .return = \"columns\"   )  df_u"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_mutate-shorthand-syntax-new-in-v0-6-2","dir":"Articles","previous_headings":"","what":"llm_mutate: shorthand syntax (NEW in v0.6.2)","title":"Tidy pipelines and structured output","text":"shorthand lets combine output column prompt one argument: multi-turn messages:","code":"df |>   llm_mutate(     category = \"Give a short category for: {text}\",     .config = cfg_cld   ) # Equivalent to: llm_mutate(category, prompt = \"Give...\", .config = cfg_cld) df |>   llm_mutate(     classified = c(       system = \"You are a text classifier. One word only.\",       user = \"Category for: {text}\"     ),     .config = cfg_openai   )"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_mutate-with--structured-flag-new-in-v0-6-2","dir":"Articles","previous_headings":"","what":"llm_mutate with .structured flag (NEW in v0.6.2)","title":"Tidy pipelines and structured output","text":"can now enable structured output directly llm_mutate() using .structured = TRUE: equivalent calling llm_mutate_structured() supports shorthand syntax.","code":"schema <- list(   type = \"object\",   properties = list(     category = list(type = \"string\"),     confidence = list(type = \"number\")   ),   required = list(\"category\", \"confidence\") )  # Using .structured = TRUE (equivalent to calling llm_mutate_structured) df |>   llm_mutate(     structured_result = \"{text}\",     .config = cfg_openai,     .structured = TRUE,     .schema = schema   )"},{"path":"https://asanaei.github.io/LLMR/articles/tidy-and-structured.html","id":"llm_mutate_structured-structured-with-shorthand-gemini","dir":"Articles","previous_headings":"","what":"llm_mutate_structured: structured with shorthand (Gemini)","title":"Tidy pipelines and structured output","text":"","code":"schema2 <- list(   type = \"object\",   properties = list(     category  = list(type = \"string\"),     rationale = list(type = \"string\")   ),   required = list(\"category\", \"rationale\"),   additionalProperties = FALSE )  # Traditional call df_s <- df |>   llm_mutate_structured(     annot,     prompt  = \"Extract category and a one-sentence rationale for: {text}\",     .config = cfg_gemini,     .schema = schema2     # Because a schema is present, fields auto-hoist; you can also pass:     # .fields = c(\"category\", \"rationale\")   )  df_s  # Or use shorthand (NEW in v0.6.2) df |>   llm_mutate_structured(     annot = \"Extract category and rationale for: {text}\",     .config = cfg_gemini,     .schema = schema2   )"},{"path":"https://asanaei.github.io/LLMR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ali Sanaei. Author, maintainer.","code":""},{"path":"https://asanaei.github.io/LLMR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Sanaei (2025). LLMR: Interface Large Language Model APIs R. R package version 0.6.3, https://github.com/asanaei/LLMR.","code":"@Manual{,   title = {LLMR: Interface for Large Language Model APIs in R},   author = {Ali Sanaei},   year = {2025},   note = {R package version 0.6.3},   url = {https://github.com/asanaei/LLMR}, }"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"llmr","dir":"","previous_headings":"","what":"Interface for Large Language Model APIs in R","title":"Interface for Large Language Model APIs in R","text":"LLMR offers unified interface Large Language Models R. supports multiple providers, robust retries, structured output, embeddings.","code":""},{"path":"https://asanaei.github.io/LLMR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Interface for Large Language Model APIs in R","text":"","code":"install.packages(\"LLMR\") # CRAN # Development: # remotes::install_github(\"asanaei/LLMR\")"},{"path":[]},{"path":"https://asanaei.github.io/LLMR/index.html","id":"configure-a-model","dir":"","previous_headings":"Quick start","what":"Configure a model","title":"Interface for Large Language Model APIs in R","text":"Store keys environment variables OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY.","code":"library(LLMR)  cfg <- llm_config( provider = \"openai\", model = \"gpt-4o-mini\", temperature = 0.2, max_tokens = 256 )"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"one-shot-generation","dir":"","previous_headings":"Quick start","what":"One-shot generation","title":"Interface for Large Language Model APIs in R","text":"","code":"r <- call_llm( config = cfg, messages = c( system = \"You are a branding expert.\", user = \"Six-word catch-phrase for eco-friendly balloons.\") )  print(r) # text + status line as.character(r) # just the text finish_reason(r) tokens(r) is_truncated(r)"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"structured-output-json-with-schema","dir":"","previous_headings":"Quick start","what":"Structured output (JSON with schema)","title":"Interface for Large Language Model APIs in R","text":"use higher-level helpers:","code":"schema <- list( type = \"object\", properties = list( label = list(type = \"string\"), score = list(type = \"number\") ), required = list(\"label\",\"score\"), additionalProperties = FALSE )  cfg_s <- enable_structured_output(cfg, schema = schema)  resp <- call_llm(cfg_s, c(system=\"Reply JSON only.\", user=\"Label and score for 'MNIST'.\")) parsed <- llm_parse_structured(resp) str(parsed) words <- c(\"excellent\",\"awful\",\"fine\")  out <- llm_fn_structured( x = words, prompt = \"Classify '{x}' and output {label, score in [0,1]} as JSON.\", .config = cfg, .schema = schema, .fields = c(\"label\",\"score\") ) out"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"embeddings","dir":"","previous_headings":"Quick start","what":"Embeddings","title":"Interface for Large Language Model APIs in R","text":"Batch embeddings:","code":"sentences <- c( one=\"Quiet rivers mirror bright skies.\", two=\"Thunder shakes the mountain path.\" )  emb_cfg <- llm_config( provider = \"voyage\", model = \"voyage-large-2\", embedding = TRUE )  emb <- call_llm(emb_cfg, sentences) |> parse_embeddings() dim(emb) emb <- get_batched_embeddings( texts = sentences, embed_config = emb_cfg, batch_size = 8 )"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"conversation-with-history","dir":"","previous_headings":"Quick start","what":"Conversation with history","title":"Interface for Large Language Model APIs in R","text":"","code":"chat <- chat_session(cfg, system = \"You teach statistics tersely.\") chat$send(\"Explain p-values in 12 words.\") chat$send(\"Now give a three-word analogy.\") print(chat)"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"parallel-runs","dir":"","previous_headings":"Quick start","what":"Parallel runs","title":"Interface for Large Language Model APIs in R","text":"","code":"setup_llm_parallel(workers = 4)  experiments <- build_factorial_experiments( configs = list(cfg), user_prompts = c(\"Summarize in one sentence: The Apollo program.\"), system_prompts= \"Be concise.\" )  res <- call_llm_par(experiments, progress = TRUE) reset_llm_parallel()"},{"path":"https://asanaei.github.io/LLMR/index.html","id":"notes","dir":"","previous_headings":"","what":"Notes","title":"Interface for Large Language Model APIs in R","text":"Generative calls return llmr_response. Coerce .character() want plain text. Structured output: enable_structured_output() selects correct provider toggle. Use llm_parse_structured() optional llm_validate_structured_col() robust local parsing validation. Embeddings: call_llm() returns provider-native list; parse_embeddings() converts numeric matrix. Robust retries: call_llm_robust() handles rate limits exponential backoff.","code":""},{"path":"https://asanaei.github.io/LLMR/index.html","id":"contributions","dir":"","previous_headings":"","what":"Contributions","title":"Interface for Large Language Model APIs in R","text":"Issues pull requests welcome. Include minimal reproducible exampleyy.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/bind_tools.html","id":null,"dir":"Reference","previous_headings":"","what":"Bind tools to a config (provider-agnostic) — bind_tools","title":"Bind tools to a config (provider-agnostic) — bind_tools","text":"Bind tools config (provider-agnostic)","code":""},{"path":"https://asanaei.github.io/LLMR/reference/bind_tools.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bind tools to a config (provider-agnostic) — bind_tools","text":"","code":"bind_tools(config, tools, tool_choice = NULL)"},{"path":"https://asanaei.github.io/LLMR/reference/bind_tools.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bind tools to a config (provider-agnostic) — bind_tools","text":"config llm_config tools list tools (name, description, parameters/input_schema) tool_choice optional tool_choice spec (provider-specific shape)","code":""},{"path":"https://asanaei.github.io/LLMR/reference/bind_tools.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bind tools to a config (provider-agnostic) — bind_tools","text":"modified llm_config","code":""},{"path":"https://asanaei.github.io/LLMR/reference/build_factorial_experiments.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Factorial Experiment Design — build_factorial_experiments","title":"Build Factorial Experiment Design — build_factorial_experiments","text":"Creates tibble experiments factorial designs want test combinations configs, messages, repetitions automatic metadata.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/build_factorial_experiments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Factorial Experiment Design — build_factorial_experiments","text":"","code":"build_factorial_experiments(   configs,   user_prompts,   system_prompts = NULL,   repetitions = 1,   config_labels = NULL,   user_prompt_labels = NULL,   system_prompt_labels = NULL )"},{"path":"https://asanaei.github.io/LLMR/reference/build_factorial_experiments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Factorial Experiment Design — build_factorial_experiments","text":"configs List llm_config objects test. user_prompts Character vector (list) user‑turn prompts. system_prompts Optional character vector system messages (recycled user prompts). Missing/NA values ignored; messages user-. repetitions Integer. Number repetitions per combination. Default 1. config_labels Character vector labels configs. NULL, uses \"provider_model\". user_prompt_labels Optional labels user prompts. system_prompt_labels Optional labels system prompts.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/build_factorial_experiments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Factorial Experiment Design — build_factorial_experiments","text":"tibble columns: config (list-column), messages (list-column), config_label, user_prompt_label, system_prompt_label, repetition. Ready use call_llm_par().","code":""},{"path":"https://asanaei.github.io/LLMR/reference/build_factorial_experiments.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Factorial Experiment Design — build_factorial_experiments","text":"","code":"if (FALSE) { # \\dontrun{   # Factorial design: 3 configs x 2 user prompts x 10 reps = 60 experiments   configs <- list(gpt4_config, claude_config, llama_config)   user_prompts <- c(\"Control prompt\", \"Treatment prompt\")    experiments <- build_factorial_experiments(     configs = configs,     user_prompts = user_prompts,     repetitions = 10,     config_labels = c(\"gpt4\", \"claude\", \"llama\"),     user_prompt_labels = c(\"control\", \"treatment\")   )    # Use with call_llm_par   results <- call_llm_par(experiments, progress = TRUE) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":null,"dir":"Reference","previous_headings":"","what":"Cache LLM API Calls — cache_llm_call","title":"Cache LLM API Calls — cache_llm_call","text":"memoised version call_llm avoid repeated identical requests.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cache LLM API Calls — cache_llm_call","text":"","code":"cache_llm_call(config, messages, verbose = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cache LLM API Calls — cache_llm_call","text":"config llm_config object llm_config. messages list message objects character vector embeddings. verbose Logical. TRUE, prints full API response (passed call_llm).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cache LLM API Calls — cache_llm_call","text":"(memoised) response object call_llm.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cache LLM API Calls — cache_llm_call","text":"Requires memoise package. Add memoise package's DESCRIPTION. Clearing cache can done via memoise::forget(cache_llm_call) restarting R session.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/cache_llm_call.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cache LLM API Calls — cache_llm_call","text":"","code":"if (FALSE) { # \\dontrun{   # Using cache_llm_call:   response1 <- cache_llm_call(my_config, list(list(role=\"user\", content=\"Hello!\")))   # Subsequent identical calls won't hit the API unless we clear the cache.   response2 <- cache_llm_call(my_config, list(list(role=\"user\", content=\"Hello!\"))) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":null,"dir":"Reference","previous_headings":"","what":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"call_llm() dispatches correct provider implementation based config$provider. supports generative chat/completions embeddings, plus simple multimodal shortcut local files.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"","code":"call_llm(config, messages, verbose = FALSE)  # S3 method for class 'ollama' call_llm(config, messages, verbose = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"config llm_config object. messages One : Plain character vector — element becomes \"user\" message. Named character vector — names roles (\"system\", \"user\", \"assistant\"). Multimodal shortcut: include one elements named \"file\" whose values local paths; consecutive {user | file} entries combined one user turn files inlined (base64) capable providers. List message objects: list(role=..., content=...). multimodal content, set content list parts like list(list(type=\"text\", text=\"...\"), list(type=\"file\", path=\"...\")). verbose Logical. TRUE, prints full parsed API response.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"Generative mode: llmr_response object. Use .character(x) get just text; print(x) shows text plus status line; use helpers finish_reason(x) tokens(x). Embedding mode: provider-native list element data; convert parse_embeddings().","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"provider-notes","dir":"Reference","previous_headings":"","what":"Provider notes","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"OpenAI-compatible: server 400 identifies bad parameter max_tokens, LLMR , unless no_change=TRUE, retry replacing max_tokens max_completion_tokens (inform via cli_alert_info). former experimental \"uncapped retry empty content\" disabled default avoid unexpected costs. Anthropic: max_tokens required; omitted LLMR uses 2048 warns. Multimodal images inlined base64. Extended thinking supported: provide thinking_budget include_thoughts = TRUE include content block type \"thinking\" response; LLMR sets beta header automatically. Gemini (REST): systemInstruction supported; user parts use text/inlineData(mimeType,data); responses set responseMimeType = \"text/plain\". Ollama (local): OpenAI-compatible endpoints http://localhost:11434/v1/*; Authorization header required. Override api_url needed. Error handling: HTTP errors raise structured conditions classes like llmr_api_param_error, llmr_api_rate_limit_error, llmr_api_server_error; see condition fields status, code, request id, (supplied) offending parameter.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"message-normalization","dir":"Reference","previous_headings":"","what":"Message normalization","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"See \"multimodal shortcut\" described messages. Internally, LLMR expands provider's native request shape tilde-expands local file paths.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"using-a-local-ollama-server","dir":"Reference","previous_headings":"","what":"Using a local Ollama server","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"Ollama provides OpenAI-compatible HTTP API localhost default. Start daemon pull model first (terminal): ollama serve (background) ollama pull llama3. configure LLMR llm_config(\"ollama\", \"llama3\", embedding = FALSE) chat llm_config(\"ollama\", \"nomic-embed-text\", embedding = TRUE) embeddings. Override endpoint api_url using default http://localhost:11434/v1/*.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Call an LLM (chat/completions or embeddings) with optional multimodal input — call_llm","text":"","code":"if (FALSE) { # \\dontrun{ ## 1) Basic generative call cfg <- llm_config(\"openai\", \"gpt-4o-mini\") call_llm(cfg, \"Say hello in Greek.\")  ## 2) Generative with rich return r <- call_llm(cfg, \"Say hello in Greek.\") r as.character(r) finish_reason(r); tokens(r)  ## 3) Anthropic extended thinking (single example) a_cfg <- llm_config(\"anthropic\", \"claude-sonnet-4-20250514\",                     max_tokens = 5000,                     thinking_budget = 16000,                     include_thoughts = TRUE) r2 <- call_llm(a_cfg, \"Compute 87*93 in your head. Give only the final number.\") # thinking (if present): r2$raw$content[[1]]$thinking # final text:            r2$raw$content[[2]]$text  ## 4) Multimodal (named-vector shortcut) msg <- c(   system = \"Answer briefly.\",   user   = \"Describe this image in one sentence.\",   file   = \"~/Pictures/example.png\" ) call_llm(cfg, msg)  ## 5) Embeddings e_cfg <- llm_config(\"voyage\", \"voyage-large-2\",                     embedding = TRUE) emb_raw <- call_llm(e_cfg, c(\"first\", \"second\")) emb_mat <- parse_embeddings(emb_raw)  ## 6) With a chat session ch <- chat_session(cfg) ch$send(\"Say hello in Greek.\")   # prints the same status line as `print.llmr_response` ch$history() } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"Broadcasts different messages using configuration parallel. Perfect batch processing different prompts consistent settings. function requires setting parallel environment using setup_llm_parallel.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"","code":"call_llm_broadcast(config, messages, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"config Single llm_config object use calls. messages character vector (element prompt) list element pre-formatted message list. ... Additional arguments passed call_llm_par (e.g., tries, verbose, progress).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"tibble columns: message_index (metadata), provider, model, model parameters, response_text, raw_response_json, success, error_message.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":"parallel-workflow","dir":"Reference","previous_headings":"","what":"Parallel Workflow","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"parallel functions require future backend configured. recommended workflow : Call setup_llm_parallel() start script. Run one parallel experiments (e.g., call_llm_broadcast()). Call reset_llm_parallel() end restore sequential processing.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_broadcast.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parallel API calls: Fixed Config, Multiple Messages — call_llm_broadcast","text":"","code":"if (FALSE) { # \\dontrun{   # Broadcast different questions   config <- llm_config(provider = \"openai\", model = \"gpt-4.1-nano\")    messages <- list(     list(list(role = \"user\", content = \"What is 2+2?\")),     list(list(role = \"user\", content = \"What is 3*5?\")),     list(list(role = \"user\", content = \"What is 10/2?\"))   )    setup_llm_parallel(workers = 4, verbose = TRUE)   results <- call_llm_broadcast(config, messages)   reset_llm_parallel(verbose = TRUE) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"Compares different configurations (models, providers, settings) using message. Perfect benchmarking across different models providers. function requires setting parallel environment using setup_llm_parallel.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"","code":"call_llm_compare(configs_list, messages, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"configs_list list llm_config objects compare. messages character vector list message objects (configs). ... Additional arguments passed call_llm_par (e.g., tries, verbose, progress).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"tibble columns: config_index (metadata), provider, model, varying model parameters, response_text, raw_response_json, success, error_message.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":"parallel-workflow","dir":"Reference","previous_headings":"","what":"Parallel Workflow","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"parallel functions require future backend configured. recommended workflow : Call setup_llm_parallel() start script. Run one parallel experiments (e.g., call_llm_broadcast()). Call reset_llm_parallel() end restore sequential processing.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_compare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parallel API calls: Multiple Configs, Fixed Message — call_llm_compare","text":"","code":"if (FALSE) { # \\dontrun{   # Compare different models   config1 <- llm_config(provider = \"openai\", model = \"gpt-4o-mini\")   config2 <- llm_config(provider = \"openai\", model = \"gpt-4.1-nano\")    configs_list <- list(config1, config2)   messages <- \"Explain quantum computing\"    setup_llm_parallel(workers = 4, verbose = TRUE)   results <- call_llm_compare(configs_list, messages)   reset_llm_parallel(verbose = TRUE) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"Processes experiments tibble row contains config message pair. core parallel processing function. Metadata columns preserved. function requires setting parallel environment using setup_llm_parallel.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"","code":"call_llm_par(   experiments,   simplify = TRUE,   tries = 10,   wait_seconds = 2,   backoff_factor = 120^(1/tries),   verbose = FALSE,   memoize = FALSE,   max_workers = NULL,   progress = FALSE,   json_output = NULL,   start_jitter = 5 )"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"experiments tibble/data.frame required list-columns 'config' (llm_config objects) 'messages' (character vector message list). simplify Whether cbind 'experiments' output data frame . tries Integer. Number retries call. Default 10. wait_seconds Numeric. Initial wait time (seconds) retry. Default 2. backoff_factor Numeric. Multiplier wait time failure. Default 3. verbose Logical. TRUE, prints progress debug information. memoize Logical. TRUE, enables caching identical requests. max_workers Integer. Maximum number parallel workers. NULL, auto-detects. progress Logical. TRUE, shows progress bar. json_output Deprecated. Raw JSON string always included raw_response_json. parameter kept backward compatibility effect. start_jitter Calls made uniformly distributed delay 0 start_jitter seconds.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"tibble containing original columns plus: response_text – assistant text (NA failure) raw_response_json – raw JSON string success, error_message finish_reason – e.g. \"stop\", \"length\", \"filter\", \"tool\", \"error:category\" sent_tokens, rec_tokens, total_tokens, reasoning_tokens response_id duration – seconds response – full llmr_response object (NA failure) response column holds llmr_response objects success, NULL failure.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":"parallel-workflow","dir":"Reference","previous_headings":"","what":"Parallel Workflow","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"parallel functions require future backend configured. recommended workflow : Call setup_llm_parallel() start script. Run one parallel experiments (e.g., call_llm_broadcast()). Call reset_llm_parallel() end restore sequential processing.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parallel LLM Processing with Tibble-Based Experiments (Core Engine) — call_llm_par","text":"","code":"if (FALSE) { # \\dontrun{ # Simple example: Compare two models on one prompt cfg1 <- llm_config(\"openai\", \"gpt-4.1-nano\") cfg2 <- llm_config(\"groq\", \"llama-3.3-70b-versatile\")  experiments <- tibble::tibble(   model_id = c(\"gpt-4.1-nano\", \"groq-llama-3.3\"),   config = list(cfg1, cfg2),   messages = \"Count the number of the letter e in this word: Freundschaftsbeziehungen \" )  setup_llm_parallel(workers = 2) results <- call_llm_par(experiments, progress = TRUE) reset_llm_parallel()  print(results[, c(\"model_id\", \"response_text\")])  } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par_structured.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel experiments with structured parsing — call_llm_par_structured","title":"Parallel experiments with structured parsing — call_llm_par_structured","text":"Enables structured output config (already set), runs, parses JSON.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par_structured.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel experiments with structured parsing — call_llm_par_structured","text":"","code":"call_llm_par_structured(experiments, schema = NULL, .fields = NULL, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_par_structured.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel experiments with structured parsing — call_llm_par_structured","text":"experiments Tibble config messages list-columns. schema Optional JSON Schema list. .fields Optional fields hoist parsed JSON (supports nested paths). ... Passed call_llm_par().","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_robust.html","id":null,"dir":"Reference","previous_headings":"","what":"Robustly Call LLM API (Simple Retry) — call_llm_robust","title":"Robustly Call LLM API (Simple Retry) — call_llm_robust","text":"Wraps call_llm handle rate-limit errors (HTTP 429 related \"Many Requests\" messages). retries call specified number times, using exponential backoff. can also choose cache responses need fresh results time.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_robust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Robustly Call LLM API (Simple Retry) — call_llm_robust","text":"","code":"call_llm_robust(   config,   messages,   tries = 5,   wait_seconds = 10,   backoff_factor = 5,   verbose = FALSE,   memoize = FALSE )"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_robust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Robustly Call LLM API (Simple Retry) — call_llm_robust","text":"config llm_config object llm_config. messages list message objects (character vector embeddings). tries Integer. Number retries giving . Default 5. wait_seconds Numeric. Initial wait time (seconds) first retry. Default 10. backoff_factor Numeric. Multiplier wait time failure. Default 5. verbose Logical. TRUE, prints full API response. memoize Logical. TRUE, calls cached avoid repeated identical requests. Default FALSE.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_robust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Robustly Call LLM API (Simple Retry) — call_llm_robust","text":"successful result call_llm, error retries fail.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_robust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Robustly Call LLM API (Simple Retry) — call_llm_robust","text":"","code":"if (FALSE) { # \\dontrun{ robust_resp <- call_llm_robust( config = llm_config(\"openai\",\"gpt-4o-mini\"), messages = list(list(role = \"user\", content = \"Hello, LLM!\")), tries = 5, wait_seconds = 10, memoize = FALSE ) print(robust_resp) as.character(robust_resp) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":null,"dir":"Reference","previous_headings":"","what":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"Sweeps different values single parameter keeping message constant. Perfect hyperparameter tuning, temperature experiments, etc. function requires setting parallel environment using setup_llm_parallel.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"","code":"call_llm_sweep(base_config, param_name, param_values, messages, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"base_config Base llm_config object modify. param_name Character. Name parameter vary (e.g., \"temperature\", \"max_tokens\"). param_values Vector. Values test parameter. messages character vector list message objects (calls). ... Additional arguments passed call_llm_par (e.g., tries, verbose, progress).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"tibble columns: swept_param_name, varied parameter column, provider, model, model parameters, response_text, raw_response_json, success, error_message.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":"parallel-workflow","dir":"Reference","previous_headings":"","what":"Parallel Workflow","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"parallel functions require future backend configured. recommended workflow : Call setup_llm_parallel() start script. Run one parallel experiments (e.g., call_llm_broadcast()). Call reset_llm_parallel() end restore sequential processing.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/call_llm_sweep.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parallel API calls: Parameter Sweep - Vary One Parameter, Fixed Message — call_llm_sweep","text":"","code":"if (FALSE) { # \\dontrun{   # Temperature sweep   config <- llm_config(provider = \"openai\", model = \"gpt-4.1-nano\")    messages <- \"What is 15 * 23?\"   temperatures <- c(0, 0.3, 0.7, 1.0, 1.5)    setup_llm_parallel(workers = 4, verbose = TRUE)   results <- call_llm_sweep(config, \"temperature\", temperatures, messages)   results |> dplyr::select(temperature, response_text)   reset_llm_parallel(verbose = TRUE) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/disable_structured_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Disable Structured Output (clean provider toggles) — disable_structured_output","title":"Disable Structured Output (clean provider toggles) — disable_structured_output","text":"Removes response_format/response_schema/response_mime_type schema tool present. Keeps user tools intact.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/disable_structured_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disable Structured Output (clean provider toggles) — disable_structured_output","text":"","code":"disable_structured_output(config)"},{"path":"https://asanaei.github.io/LLMR/reference/disable_structured_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Disable Structured Output (clean provider toggles) — disable_structured_output","text":"config llm_config","code":""},{"path":"https://asanaei.github.io/LLMR/reference/enable_structured_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Enable Structured Output (Provider-Agnostic) — enable_structured_output","title":"Enable Structured Output (Provider-Agnostic) — enable_structured_output","text":"Turn structured output model configuration. Supports OpenAI‑compatible providers (OpenAI, Groq, Together, x.ai, DeepSeek), Anthropic, Gemini.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/enable_structured_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Enable Structured Output (Provider-Agnostic) — enable_structured_output","text":"","code":"enable_structured_output(   config,   schema = NULL,   name = \"llmr_schema\",   method = c(\"auto\", \"json_mode\", \"tool_call\"),   strict = TRUE )"},{"path":"https://asanaei.github.io/LLMR/reference/enable_structured_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Enable Structured Output (Provider-Agnostic) — enable_structured_output","text":"config llm_config object. schema named list representing JSON Schema. NULL, OpenAI-compatible providers enforce JSON object; Gemini switches JSON mime type; Anthropic injects tool schema supplied. name Character. Schema/tool name providers requiring one. Default \"llmr_schema\". method One c(\"auto\",\"json_mode\",\"tool_call\"). \"auto\" chooses best per provider. rarely need change . strict Logical. Request strict validation supported (OpenAI-compatible).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/enable_structured_output.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Enable Structured Output (Provider-Agnostic) — enable_structured_output","text":"Modified llm_config.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/get_batched_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings in Batches — get_batched_embeddings","title":"Generate Embeddings in Batches — get_batched_embeddings","text":"wrapper function processes list texts batches generate embeddings, avoiding rate limits. function calls call_llm_robust batch stitches results together parses (using parse_embeddings) return numeric matrix.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/get_batched_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings in Batches — get_batched_embeddings","text":"","code":"get_batched_embeddings(texts, embed_config, batch_size = 50, verbose = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/get_batched_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings in Batches — get_batched_embeddings","text":"texts Character vector texts embed. named, names used row names output matrix. embed_config llm_config object configured embeddings. batch_size Integer. Number texts process batch. Default 50. verbose Logical. TRUE, prints progress messages. Default TRUE.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/get_batched_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings in Batches — get_batched_embeddings","text":"numeric matrix row embedding vector corresponding text. Columns named v1, v2, ..., vK K embedding dimension. embedding fails certain texts, rows filled NA values. matrix always number rows input texts. Returns NULL embeddings successfully generated.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/get_batched_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Embeddings in Batches — get_batched_embeddings","text":"","code":"if (FALSE) { # \\dontrun{   # Basic usage   texts <- c(\"Hello world\", \"How are you?\", \"Machine learning is great\")   names(texts) <- c(\"greeting\", \"question\", \"statement\")    embed_cfg <- llm_config(     provider = \"voyage\",     model = \"voyage-large-2-instruct\",     embedding = TRUE,     api_key = Sys.getenv(\"VOYAGE_API_KEY\")   )    embeddings <- get_batched_embeddings(     texts = texts,     embed_config = embed_cfg,     batch_size = 2   ) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat Session Object and Methods — llm_chat_session","title":"Chat Session Object and Methods — llm_chat_session","text":"Create interact stateful chat session object retains message history. documentation page covers constructor function chat_session() well S3 methods llm_chat_session class.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat Session Object and Methods — llm_chat_session","text":"","code":"chat_session(config, system = NULL, ...)  # S3 method for class 'llm_chat_session' as.data.frame(x, ...)  # S3 method for class 'llm_chat_session' summary(object, ...)  # S3 method for class 'llm_chat_session' head(x, n = 6L, width = getOption(\"width\") - 15, ...)  # S3 method for class 'llm_chat_session' tail(x, n = 6L, width = getOption(\"width\") - 15, ...)  # S3 method for class 'llm_chat_session' print(x, width = getOption(\"width\") - 15, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat Session Object and Methods — llm_chat_session","text":"config llm_config generative model (embedding = FALSE). system Optional system prompt inserted beginning. ... Default arguments forwarded every call_llm_robust() call (e.g. verbose = TRUE). x, object llm_chat_session object. n Number turns display. width Character width truncating long messages.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat Session Object and Methods — llm_chat_session","text":"chat_session(), object class llm_chat_session. methods return titles state.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Chat Session Object and Methods — llm_chat_session","text":"chat_session object provides simple way hold conversation generative model. wraps call_llm_robust() benefit retry logic, caching, error logging.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"how-it-works","dir":"Reference","previous_headings":"","what":"How it works","title":"Chat Session Object and Methods — llm_chat_session","text":"private environment stores running list list(role, content) messages. $send() history sent full model. Provider-agnostic token counts extracted JSON response.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Chat Session Object and Methods — llm_chat_session","text":"$send(text, ..., role = \"user\") Append message (default role \"user\"), query model, print assistant's reply, invisibly return . $send_structured(text, schema, ..., role = \"user\", .fields = NULL, .validate_local = TRUE) Send message structured-output enabled using schema, append assistant's reply, parse JSON (optionally validate locally .validate_local = TRUE), returning parsed result invisibly. $history() Raw list messages. $history_df() Two-column data frame (role, content). $tokens_sent()/$tokens_received() Running token totals. $reset() Clear history (retains optional system message).","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/llm_chat_session.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat Session Object and Methods — llm_chat_session","text":"","code":"if (interactive()) {   cfg  <- llm_config(\"openai\", \"gpt-4o-mini\")   chat <- chat_session(cfg, system = \"Be concise.\")   chat$send(\"Who invented the moon?\")   chat$send(\"Explain why in one short sentence.\")   chat           # print() shows a summary and first 10 turns   summary(chat)  # stats   tail(chat, 2)   as.data.frame(chat) }"},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an LLM configuration (provider-agnostic) — llm_config","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"llm_config() builds provider-agnostic configuration object call_llm() (friends) understand. can pass provider-specific parameters via ...; LLMR forwards -, safe conveniences.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"","code":"llm_config(   provider,   model,   api_key = NULL,   troubleshooting = FALSE,   base_url = NULL,   embedding = NULL,   no_change = FALSE,   ... )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"provider Character scalar. One : \"openai\", \"anthropic\", \"gemini\", \"groq\", \"together\", \"voyage\" (embeddings ), \"deepseek\", \"xai\", \"ollama\". model Character scalar. Model name understood chosen provider. (e.g., \"gpt-4o-mini\", \"o4-mini\", \"claude-3.7\", \"gemini-2.0-flash\", etc.) api_key Character scalar. Provider API key. troubleshooting Logical. TRUE, prints full request payloads (including API key!) debugging. Use extreme caution. base_url Optional character. Back-compat alias; supplied stored api_url model_params overrides default endpoint. embedding NULL (default), TRUE, FALSE. TRUE, call routed provider's embeddings API; FALSE, chat API. NULL, LLMR infers embeddings model contains \"embedding\". no_change Logical. TRUE, LLMR never auto-renames/adjusts provider parameters. FALSE (default), well-known compatibility shims may apply (e.g., renaming OpenAI's max_tokens → max_completion_tokens server hint; see call_llm() notes). ... Additional provider-specific parameters (e.g., temperature, top_p, max_tokens, top_k, repetition_penalty, reasoning_effort, api_url, etc.). Values forwarded verbatim unless documented shims apply. Anthropic extended thinking, supply thinking_budget (canonical; mapped thinking.budget_tokens) together include_thoughts = TRUE request thinking block response.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"object class c(\"llm_config\", provider). Fields: provider, model, api_key, troubleshooting, embedding, no_change, model_params (named list extras).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"temperature-range-clamping","dir":"Reference","previous_headings":"","what":"Temperature range clamping","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"Anthropic temperatures must [0, 1]; others [0, 2]. --range values clamped warning.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"endpoint-overrides","dir":"Reference","previous_headings":"","what":"Endpoint overrides","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"can pass api_url (base_url= alias) ... point gateways compatible proxies.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/llm_config.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an LLM configuration (provider-agnostic) — llm_config","text":"","code":"if (FALSE) { # \\dontrun{ # Basic OpenAI config cfg <- llm_config(\"openai\", \"gpt-4o-mini\", temperature = 0.7, max_tokens = 300)  # Generative call returns an llmr_response object r <- call_llm(cfg, \"Say hello in Greek.\") print(r) as.character(r)  # Embeddings (inferred from the model name) e_cfg <- llm_config(\"gemini\", \"text-embedding-004\")  # Force embeddings even if model name does not contain \"embedding\" e_cfg2 <- llm_config(\"voyage\", \"voyage-large-2\", embedding = TRUE) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply an LLM prompt over vectors/data frames — llm_fn","title":"Apply an LLM prompt over vectors/data frames — llm_fn","text":"Apply LLM prompt vectors/data frames","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply an LLM prompt over vectors/data frames — llm_fn","text":"","code":"llm_fn(   x,   prompt,   .config,   .system_prompt = NULL,   ...,   .return = c(\"text\", \"columns\", \"object\") )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply an LLM prompt over vectors/data frames — llm_fn","text":"x character vector data.frame/tibble. prompt glue template string. data-frame may reference columns ({col}); vector placeholder {x}. .config llm_config object. .system_prompt Optional system message (character scalar). ... Passed unchanged call_llm_broadcast() (e.g. tries, progress, verbose). .return One c(\"text\",\"columns\",\"object\"). \"columns\" returns tibble diagnostic columns; \"text\" returns character vector; \"object\" returns list llmr_response (NA failure).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply an LLM prompt over vectors/data frames — llm_fn","text":"generative mode: .return = \"text\": character vector .return = \"columns\": tibble diagnostics .return = \"object\": list llmr_response (NA failure) embedding mode, always numeric matrix.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Apply an LLM prompt over vectors/data frames — llm_fn","text":"","code":"if (interactive()) {   words <- c(\"excellent\",\"awful\")   cfg <- llm_config(\"openai\",\"gpt-4o-mini\", temperature = 0)   llm_fn(words, \"Classify '{x}' as Positive/Negative.\",          cfg,          .system_prompt=\"One word.\",          .return=\"columns\") }"},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn_structured.html","id":null,"dir":"Reference","previous_headings":"","what":"Vectorized structured-output LLM — llm_fn_structured","title":"Vectorized structured-output LLM — llm_fn_structured","text":"Schema-first variant llm_fn(). enables structured output config, calls model via call_llm_broadcast(), parses JSON, optionally validates.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn_structured.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Vectorized structured-output LLM — llm_fn_structured","text":"","code":"llm_fn_structured(   x,   prompt,   .config,   .system_prompt = NULL,   ...,   .schema = NULL,   .fields = NULL,   .local_only = FALSE,   .validate_local = TRUE )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_fn_structured.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Vectorized structured-output LLM — llm_fn_structured","text":"x character vector data.frame/tibble. prompt glue template string. data-frame may reference columns ({col}); vector placeholder {x}. .config llm_config object. .system_prompt Optional system message (character scalar). ... Passed unchanged call_llm_broadcast() (e.g. tries, progress, verbose). .schema Optional JSON Schema list; NULL, JSON object enforced. .fields Optional fields hoist parsed JSON (supports nested paths). .local_only TRUE, send schema provider (parse/validate locally). .validate_local TRUE .schema provided, validate locally.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":null,"dir":"Reference","previous_headings":"","what":"Mutate a data frame with LLM output — llm_mutate","title":"Mutate a data frame with LLM output — llm_mutate","text":"Adds one columns .data produced Large-Language-Model.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mutate a data frame with LLM output — llm_mutate","text":"","code":"llm_mutate(   .data,   output,   prompt = NULL,   .messages = NULL,   .config,   .system_prompt = NULL,   .before = NULL,   .after = NULL,   .return = c(\"columns\", \"text\", \"object\"),   .structured = FALSE,   .schema = NULL,   .fields = NULL,   ... )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mutate a data frame with LLM output — llm_mutate","text":".data data.frame / tibble. output Unquoted name becomes new column (generative) prefix embedding columns. prompt Optional glue template string single user turn; reference columns .data (e.g. \"{id}. {question}\\nContext: {context}\"). Ignored .messages supplied. .messages Optional named character vector glue templates build multi-turn message, using roles c(\"system\",\"user\",\"assistant\",\"file\"). Values glue templates evaluated per-row; can reference multiple columns. multimodal, use role \"file\" column containing path template. .config llm_config object (generative embedding). .system_prompt Optional system message sent every request .messages include system entry. ., .Standard dplyr::relocate helpers controlling generated column(s) placed. .return One c(\"columns\",\"text\",\"object\"). generative mode, controls results added. \"columns\" (default) adds text plus diagnostic columns; \"text\" adds single text column; \"object\" adds list-column llmr_response objects. .structured Logical. TRUE, enables structured JSON output automatic parsing. Requires .schema provided. enabled, equivalent calling llm_mutate_structured(). Default FALSE. .schema Optional JSON Schema (R list). .structured = TRUE, schema sent provider validation used local parsing. NULL, JSON mode enabled (strict schema validation). .fields Optional character vector fields extract parsed JSON. Supports nested paths (e.g., \"user.name\" \"/data/items/0\"). NULL .schema provided, auto-extracts top-level schema properties. Set FALSE skip field extraction entirely. ... Passed underlying calls: call_llm_broadcast() generative mode, get_batched_embeddings() embedding mode.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mutate a data frame with LLM output — llm_mutate","text":".data new column(s) appended.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mutate a data frame with LLM output — llm_mutate","text":"Multi-column injection: templating NA-safe (NA -> empty string). Multi-turn templating: supply .messages = c(system=..., user=..., file=...). Duplicate role names allowed (e.g., two user turns). Generative mode: one request per row via call_llm_broadcast(). Parallel execution follows active future plan; see setup_llm_parallel(). Embedding mode: per-row text embedded via get_batched_embeddings(). Result expands numeric columns named paste0(<output>, 1:N). rows fail embed, single <output>1 column NA returned. Diagnostic columns use suffixes: _finish, _sent, _rec, _tot, _reason, _ok, _err, _id, _status, _ecode, _param, _t.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"shorthand","dir":"Reference","previous_headings":"","what":"Shorthand","title":"Mutate a data frame with LLM output — llm_mutate","text":"can supply output column prompt one argument: equivalent :","code":"df |> llm_mutate(answer = \"{question} (hint: {hint})\", .config = cfg) df |> llm_mutate(answer = c(system = \"One word.\", user = \"{question}\"), .config = cfg) df |> llm_mutate(answer, prompt = \"{question} (hint: {hint})\", .config = cfg) df |> llm_mutate(answer, .messages = c(system = \"One word.\", user = \"{question}\"), .config = cfg)"},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mutate a data frame with LLM output — llm_mutate","text":"","code":"if (FALSE) { # \\dontrun{ library(dplyr)  df <- tibble::tibble(   id       = 1:2,   question = c(\"Capital of France?\", \"Author of 1984?\"),   hint     = c(\"European city\", \"English novelist\") )  cfg <- llm_config(\"openai\", \"gpt-4o-mini\",                   temperature = 0)  # Generative: single-turn with multi-column injection df |>   llm_mutate(     answer,     prompt = \"{question} (hint: {hint})\",     .config = cfg,     .system_prompt = \"Respond in one word.\"   )  # Generative: multi-turn via .messages (system + user) df |>   llm_mutate(     advice,     .messages = c(       system = \"You are a helpful zoologist. Keep answers short.\",       user   = \"What is a key fact about this? {question} (hint: {hint})\"     ),     .config = cfg   )  # Multimodal: include an image path with role 'file' pics <- tibble::tibble(   img    = c(\"inst/extdata/cat.png\", \"inst/extdata/dog.jpg\"),   prompt = c(\"Describe the image.\", \"Describe the image.\") ) pics |>   llm_mutate(     vision_desc,     .messages = c(user = \"{prompt}\", file = \"{img}\"),     .config = llm_config(\"openai\",\"gpt-4.1-mini\")   )  # Embeddings: output name becomes the prefix of embedding columns emb_cfg <- llm_config(\"voyage\", \"voyage-3.5-lite\",                       embedding = TRUE) df |>   llm_mutate(     vec,     prompt  = \"{question}\",     .config = emb_cfg,     .after  = id   )  # Structured output: using .structured = TRUE (equivalent to llm_mutate_structured) schema <- list(   type = \"object\",   properties = list(     answer = list(type = \"string\"),     confidence = list(type = \"number\")   ),   required = list(\"answer\", \"confidence\") )  df |>   llm_mutate(     result,     prompt = \"{question}\",     .config = cfg,     .structured = TRUE,     .schema = schema   )  # Structured with shorthand df |>   llm_mutate(     result = \"{question}\",     .config = cfg,     .structured = TRUE,     .schema = schema   ) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate_structured.html","id":null,"dir":"Reference","previous_headings":"","what":"Data-frame mutate with structured output — llm_mutate_structured","title":"Data-frame mutate with structured output — llm_mutate_structured","text":"Drop-schema-first variant llm_mutate(). Produces parsed columns.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate_structured.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data-frame mutate with structured output — llm_mutate_structured","text":"","code":"llm_mutate_structured(   .data,   output,   prompt = NULL,   .messages = NULL,   .config,   .system_prompt = NULL,   .before = NULL,   .after = NULL,   .schema = NULL,   .fields = NULL,   ... )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate_structured.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data-frame mutate with structured output — llm_mutate_structured","text":".data data.frame / tibble. output Unquoted name becomes new column (generative) prefix embedding columns. prompt Optional glue template string single user turn; reference columns .data (e.g. \"{id}. {question}\\nContext: {context}\"). Ignored .messages supplied. .messages Optional named character vector glue templates build multi-turn message, using roles c(\"system\",\"user\",\"assistant\",\"file\"). Values glue templates evaluated per-row; can reference multiple columns. multimodal, use role \"file\" column containing path template. .config llm_config object (generative embedding). .system_prompt Optional system message sent every request .messages include system entry. ., .Standard dplyr::relocate helpers controlling generated column(s) placed. .schema Optional JSON Schema (R list). provided, schema sent provider strict validation used local parsing. NULL, JSON mode enabled (strict schema validation). schema follow JSON Schema specification (e.g., type, properties, required). .fields Optional character vector fields extract parsed JSON. Supports: Character vector: c(\"name\", \"score\") - extract fields Named vector: c(person_name = \"name\", rating = \"score\") - extract rename Nested paths: c(\"user.name\", \"/data/items/0\") - dot notation JSON Pointer NULL (default): auto-extracts top-level properties .schema FALSE: skip field extraction (keep structured_data list-column) ... Passed underlying calls: call_llm_broadcast() generative mode, get_batched_embeddings() embedding mode.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_mutate_structured.html","id":"shorthand-syntax","dir":"Reference","previous_headings":"","what":"Shorthand syntax","title":"Data-frame mutate with structured output — llm_mutate_structured","text":"Like llm_mutate(), function supports shorthand syntax:","code":"df |> llm_mutate_structured(result = \"{text}\", .schema = schema) df |> llm_mutate_structured(result = c(system = \"Be brief.\", user = \"{text}\"), .schema = schema)"},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse structured output emitted by an LLM — llm_parse_structured","title":"Parse structured output emitted by an LLM — llm_parse_structured","text":"Robustly parses LLM's structured output (JSON). Works character scalars llmr_response. Strips code fences first, tries strict parsing, attempts extract largest balanced {...} [...].","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse structured output emitted by an LLM — llm_parse_structured","text":"","code":"llm_parse_structured(x, strict_only = FALSE, simplify = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse structured output emitted by an LLM — llm_parse_structured","text":"x Character llmr_response. strict_only TRUE, attempt recovery via substring extraction. simplify Logical passed jsonlite::fromJSON (simplifyVector = FALSE FALSE).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse structured output emitted by an LLM — llm_parse_structured","text":"parsed R object (list), NULL failure.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse structured output emitted by an LLM — llm_parse_structured","text":"return contract list--NULL; scalar-JSON treated failure. Numerics coerced double stability.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured_col.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","title":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","text":"Extracts fields column containing structured JSON (string list) appends new columns. Adds structured_ok (logical) structured_data (list).","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured_col.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","text":"","code":"llm_parse_structured_col(   .data,   fields,   structured_col = \"response_text\",   prefix = \"\",   allow_list = TRUE )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured_col.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","text":".data data.frame/tibble fields Character vector fields named vector (dest_name = path). structured_col Column name parse . Default \"response_text\". prefix Optional prefix new columns. allow_list Logical. TRUE (default), non-scalar values (arrays/objects) hoisted list-columns instead dropped. FALSE, scalar fields hoisted non-scalars become NA.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured_col.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","text":".data diagnostics one new column per requested field.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_parse_structured_col.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse structured fields from a column into typed vectors — llm_parse_structured_col","text":"Supports nested-path extraction via dot/bracket paths (e.g., .b[0].c) JSON Pointer (//b/0/c). allow_list = TRUE, non-scalar values become list-columns; otherwise yield NA scalars hoisted.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_validate_structured_col.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate structured JSON objects against a JSON Schema (locally) — llm_validate_structured_col","title":"Validate structured JSON objects against a JSON Schema (locally) — llm_validate_structured_col","text":"Adds structured_valid (logical) structured_error (chr) validating row's structured_data schema. provider calls made.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llm_validate_structured_col.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate structured JSON objects against a JSON Schema (locally) — llm_validate_structured_col","text":"","code":"llm_validate_structured_col(   .data,   schema,   structured_list_col = \"structured_data\" )"},{"path":"https://asanaei.github.io/LLMR/reference/llm_validate_structured_col.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate structured JSON objects against a JSON Schema (locally) — llm_validate_structured_col","text":".data data.frame structured_data list-column. schema JSON Schema (R list) structured_list_col Column name parsed JSON. Default \"structured_data\".","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":null,"dir":"Reference","previous_headings":"","what":"LLMR Response Object — llmr_response","title":"LLMR Response Object — llmr_response","text":"lightweight S3 container generative model calls. standardizes finish reasons token usage across providers keeps raw response advanced users. Returns standardized finish reason llmr_response. Returns list token counts llmr_response. Convenience check truncation due token limits.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"LLMR Response Object — llmr_response","text":"","code":"finish_reason(x)  tokens(x)  is_truncated(x)  # S3 method for class 'llmr_response' as.character(x, ...)  # S3 method for class 'llmr_response' print(x, ...)"},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"LLMR Response Object — llmr_response","text":"x llmr_response object. ... Ignored.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"LLMR Response Object — llmr_response","text":"length-1 character vector NA_character_. list list(sent, rec, total, reasoning). Missing values NA. TRUE truncated, otherwise FALSE.","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"LLMR Response Object — llmr_response","text":"text: character scalar. Assistant reply. provider: character. Provider id (e.g., \"openai\", \"gemini\"). model: character. Model id. finish_reason: one \"stop\", \"length\", \"filter\", \"tool\", \"\". usage: list integers sent, rec, total, reasoning (available). response_id: provider’s response identifier present. duration_s: numeric seconds request parse. raw: parsed provider JSON (list). raw_json: raw JSON string.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"printing","dir":"Reference","previous_headings":"","what":"Printing","title":"LLMR Response Object — llmr_response","text":"print() shows text, compact status line model, finish reason, token counts, terse hint truncated filtered.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"coercion","dir":"Reference","previous_headings":"","what":"Coercion","title":"LLMR Response Object — llmr_response","text":".character() extracts text object remains drop-code expects character return.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"see-also","dir":"Reference","previous_headings":"","what":"See also","title":"LLMR Response Object — llmr_response","text":"call_llm(), call_llm_robust(), llm_chat_session(), llm_config(), llm_mutate(), llm_fn()","code":""},{"path":"https://asanaei.github.io/LLMR/reference/llmr_response.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"LLMR Response Object — llmr_response","text":"","code":"# Minimal fabricated example (no network): r <- structure(   list(     text = \"Hello!\",     provider = \"openai\",     model = \"demo\",     finish_reason = \"stop\",     usage = list(sent = 12L, rec = 5L, total = 17L, reasoning = NA_integer_),     response_id = \"resp_123\",     duration_s = 0.012,     raw = list(choices = list(list(message = list(content = \"Hello!\")))),     raw_json = \"{}\"   ),   class = \"llmr_response\" ) as.character(r) finish_reason(r) tokens(r) print(r) if (FALSE) { # \\dontrun{ fr <- finish_reason(r) } # } if (FALSE) { # \\dontrun{ u <- tokens(r) u$total } # } if (FALSE) { # \\dontrun{ if (is_truncated(r)) message(\"Increase max_tokens\") } # }"},{"path":"https://asanaei.github.io/LLMR/reference/log_llm_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Log LLMR Errors — log_llm_error","title":"Log LLMR Errors — log_llm_error","text":"Logs error timestamp troubleshooting.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/log_llm_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log LLMR Errors — log_llm_error","text":"","code":"log_llm_error(err)"},{"path":"https://asanaei.github.io/LLMR/reference/log_llm_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log LLMR Errors — log_llm_error","text":"err error object.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/log_llm_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log LLMR Errors — log_llm_error","text":"Invisibly returns NULL.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/log_llm_error.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log LLMR Errors — log_llm_error","text":"","code":"if (FALSE) { # \\dontrun{   # Example of logging an error by catching a failure:   # Use a deliberately fake API key to force an error   config_test <- llm_config(     provider = \"openai\",     model = \"gpt-3.5-turbo\",     api_key = \"FAKE_KEY\",     temperature = 0.5,     top_p = 1,     max_tokens = 30   )    tryCatch(     call_llm(config_test, list(list(role = \"user\", content = \"Hello world!\"))),     error = function(e) log_llm_error(e)   ) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/parse_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","title":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","text":"Converts embedding response data numeric matrix.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/parse_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","text":"","code":"parse_embeddings(embedding_response)"},{"path":"https://asanaei.github.io/LLMR/reference/parse_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","text":"embedding_response response returned embedding API call.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/parse_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","text":"numeric matrix embeddings column names sequence numbers.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/parse_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Embedding Response into a Numeric Matrix — parse_embeddings","text":"","code":"if (FALSE) { # \\dontrun{   text_input <- c(\"Political science is a useful subject\",                   \"We love sociology\",                   \"German elections are different\",                   \"A student was always curious.\")    # Configure the embedding API provider (example with Voyage API)   voyage_config <- llm_config(     provider = \"voyage\",     model = \"voyage-large-2\",     api_key = Sys.getenv(\"VOYAGE_API_KEY\")   )    embedding_response <- call_llm(voyage_config, text_input)   embeddings <- parse_embeddings(embedding_response)   # Additional processing:   embeddings |> cor() |> print() } # }"},{"path":"https://asanaei.github.io/LLMR/reference/reset_llm_parallel.html","id":null,"dir":"Reference","previous_headings":"","what":"Reset Parallel Environment — reset_llm_parallel","title":"Reset Parallel Environment — reset_llm_parallel","text":"Resets future plan sequential processing.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/reset_llm_parallel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reset Parallel Environment — reset_llm_parallel","text":"","code":"reset_llm_parallel(verbose = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/reset_llm_parallel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reset Parallel Environment — reset_llm_parallel","text":"verbose Logical. TRUE, prints reset information.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/reset_llm_parallel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reset Parallel Environment — reset_llm_parallel","text":"Invisibly returns future plan place resetting sequential.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/reset_llm_parallel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reset Parallel Environment — reset_llm_parallel","text":"","code":"if (FALSE) { # \\dontrun{   # Setup parallel processing   old_plan <- setup_llm_parallel(workers = 2)    # Do some parallel work...    # Reset to sequential   reset_llm_parallel(verbose = TRUE)    # Optionally restore the specific old_plan if it was non-sequential   # future::plan(old_plan) } # }"},{"path":"https://asanaei.github.io/LLMR/reference/setup_llm_parallel.html","id":null,"dir":"Reference","previous_headings":"","what":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","title":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","text":"Convenience function set future plan optimal LLM parallel processing. Automatically detects system capabilities sets appropriate defaults.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/setup_llm_parallel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","text":"","code":"setup_llm_parallel(workers = NULL, strategy = NULL, verbose = FALSE)"},{"path":"https://asanaei.github.io/LLMR/reference/setup_llm_parallel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","text":"workers Integer. Number workers use. NULL, auto-detects optimal number (availableCores - 1, capped 8). called setup_llm_parallel(4), single numeric positional argument interpreted workers. strategy Character. future strategy use. Options: \"multisession\", \"multicore\", \"sequential\". NULL (default), automatically chooses \"multisession\". verbose Logical. TRUE, prints setup information.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/setup_llm_parallel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","text":"Invisibly returns previous future plan.","code":""},{"path":"https://asanaei.github.io/LLMR/reference/setup_llm_parallel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Setup Parallel Environment for LLM Processing — setup_llm_parallel","text":"","code":"if (FALSE) { # \\dontrun{   # Automatic setup   setup_llm_parallel()    # Manual setup with specific workers   setup_llm_parallel(workers = 4, verbose = TRUE)    # Force sequential processing for debugging   setup_llm_parallel(strategy = \"sequential\")    # Restore old plan if needed   reset_llm_parallel() } # }"},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"llmr-063","dir":"Changelog","previous_headings":"","what":"LLMR 0.6.3","title":"LLMR 0.6.3","text":"CRAN release: 2025-10-11","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"improvements-0-6-3","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LLMR 0.6.3","text":"ollama now supported provider, generative embedding models. , course, requires local ollama server .","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"bug-fixes-0-6-3","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"LLMR 0.6.3","text":"get_batched_embeddings giving ugly column names. Now go v1 vn","code":""},{"path":[]},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"new-features-0-6-2","dir":"Changelog","previous_headings":"","what":"New Features","title":"LLMR 0.6.2","text":"setup_llm_parallel() changed parameter order accept numeric positional argument workers. llm_mutate() adds shorthand form: llm_mutate(answer = \"<prompt>\" | c(system=..., user=...), .config=...). llm_mutate() gains .structured flag: set .structured = TRUE enable JSON output automatic parsing (equivalent calling llm_mutate_structured()). llm_mutate_structured() now supports shorthand syntax: llm_mutate_structured(result = \"{text}\", .schema = schema).","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"improvements-0-6-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"LLMR 0.6.2","text":"Enhanced .fields documentation clarify auto-extraction behavior nested path support. Clarified .schema = NULL enables JSON mode without strict schema validation. Added comprehensive examples demonstrating new structured output features vignettes.","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"bug-fixes-0-6-2","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"LLMR 0.6.2","text":"Removed unused internal function .category_from_condition parallel utilities. Fixed build_factorial_experiments() documentation correctly describe return value columns. Corrected call_llm_par() default value backoff_factor parameter (now correctly documented 3). Added missing @importFrom purrr declarations imported functions.","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"llmr-061","dir":"Changelog","previous_headings":"","what":"LLMR 0.6.1","title":"LLMR 0.6.1","text":"Fixed bug affected claude calls.","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"llmr-060","dir":"Changelog","previous_headings":"","what":"LLMR 0.6.0","title":"LLMR 0.6.0","text":"CRAN release: 2025-08-26","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"breaking-changes-0-6-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"LLMR 0.6.0","text":"Returns objects: call_llm() generative mode now returns llmr_response object default. Use .character(x) extract text; print(x) shows concise status line; helpers include finish_reason(), tokens(), is_truncated(). Legacy json= arguments removed. Generative calls always return llmr_response.","code":""},{"path":"https://asanaei.github.io/LLMR/news/index.html","id":"new-0-6-0","dir":"Changelog","previous_headings":"","what":"New","title":"LLMR 0.6.0","text":"can give key name instead actual token Even actual token provided, immediately turned system variable stored directly json output fixed structure now possible. llm_mutate can inject multiple pieces text columns.","code":""}]
